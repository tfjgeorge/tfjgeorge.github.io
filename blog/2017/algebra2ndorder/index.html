<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The algebra of second order methods in neural networks | Thomas George </title> <meta name="author" content="Thomas George"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tfjgeorge.github.io/blog/2017/algebra2ndorder/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Thomas</span> George </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/software/">software </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The algebra of second order methods in neural networks</h1> <p class="post-meta"> Created in October 29, 2017 </p> <p class="post-tags"> <a href="/blog/2017"> <i class="fa-solid fa-calendar fa-sm"></i> 2017 </a>   ·   <a href="/blog/category/note"> <i class="fa-solid fa-tag fa-sm"></i> note</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This note gives the derivations for the inverse of 2 different but related 2nd order matrices: the Fisher Information Matrix, and the Gauss-Newton approximation of the Hessian. In particular we highlight 2 centering properties that follow from the local structure of those matrices:</p> <ul> <li> we should always use a centered update for weight matrices, even if it does not follow the gradient direction (see section <a class="Reference" href="#subsec:Updating-the-weight">4.2↓</a>) </li> <li> we should normalize using the (centered) covariance matrix of the activation of each layer (see section <a class="Reference" href="#subsec:KFAC-inversion">3.2↓</a>) </li> </ul> <p>Along the way, we describe the derivation of an approximate method using the properties of the Kronecker product known as KFAC <span class="bibcites">[<a class="bibliocite" name="cite-2" href="#biblio-2"><span class="bib-index">2</span></a>]</span> with corresponding parameter updates, and we give a motivation for the centering trick used in Natural Neural Networks <span class="bibcites">[<a class="bibliocite" name="cite-1" href="#biblio-1"><span class="bib-index">1</span></a>]</span>.</p> <h2 id="notations-and-problem-statement">Notations and problem statement</h2> <p>We denote by \(f\left(x;\theta\right)\) the output of a fully connected neural network parametrized by its weight matrices and bias vectors grouped in a vector of paramters \(\theta\). Let us denote by \(\ell\left(f\left(x;\theta\right),y\left(x\right)\right)\) a loss function between the value given by the model \(f\left(x;\theta\right)\) and the true value \(y\left(x\right)\). \(\mathcal{L}\) is the empirical risk on a train set of \(n\) examples: \(\mathcal{L}\left(\theta\right)=\frac{1}{n}\sum_{i}\ell\left(f\left(x_{i};\theta\right),y\left(x_{i}\right)\right)=\frac{1}{n}\sum_{i}\ell\left(x_{i}\right)\) where we denoted \(\ell\left(x_{i}\right)=\ell\left(f\left(x_{i};\theta\right),y\left(x_{i}\right)\right)\) to simplify notations.</p> <p>Suppose a second order update \(\theta^{t+1}=\theta^{t}-\lambda G^{-1}\nabla_{\theta}\mathcal{L}\). \(G\) can be the Hessian matrix or an approximation given by Gauss Newton. \(G\) can also be the Fisher Information Matrix and in this case the update is called the natural gradient. By writing the expression for Gauss-Newton and Fisher, we observe that they share a similar structure:</p> \[\begin{eqnarray*} GN &amp; = &amp; \mathbb{E}_{p\left(x\right)}\left[J^{T}\frac{\partial^{2}\ell\left(x\right)}{\partial f^{2}}J\right] \end{eqnarray*}\] \[\begin{eqnarray*} F &amp; = &amp; \mathbb{E}_{p\left(x\right)}\left[J^{T}D\left(x\right)J\right] \end{eqnarray*}\] <p>\(J=\frac{\partial f\left(x;\theta\right)}{\partial\theta}\) is the jacobian matrix of the output of the network, with respect to the parameters \(\theta\). It is of size \(n_{output}\times n_{parameters}\). For a small change \(\Delta\theta\) it is a first order measure of the change in the value of \(f\left(x\right)\) or more precisely \(f\left(x;\theta+\Delta\theta\right)\approx f\left(x;\theta\right)+J\Delta\theta\). The expression for the Fisher Information Matrix is given by <span class="bibcites">[<a class="bibliocite" name="cite-3" href="#biblio-3"><span class="bib-index">3</span></a>]</span>. Without loss of generality we denote both matrices by:\(\begin{eqnarray*} G &amp; = &amp; \mathbb{E}\left[J^{T}DJ\right] \end{eqnarray*}\)</p> <h2 id="local-expression-for-the-matrix">Local expression for the matrix</h2> <p>The matrix \(G\) has size \(n_{parameters}\times n_{parameters}\). For a typical neural network with several millions of parameters it is untractable to store and to invert. We usually approximate it as block diagonal, where each block is a square matrix of the size of the number of scalar parameter values for a layer. With this structure, we can invert each block separately and apply the update layer by layer: \(\theta_{l}^{t+1}=\theta_{l}^{t}-\lambda G_{l}^{-1}\nabla_{\theta_{l}}\mathcal{L}\). Let us now give an exact expression for this smaller matrix \(G_{l}\) and its inverse \(G_{l}^{-1}\). We call it local it the sense that it is local to a layer.</p> <h3 id="stacking-the-parameters">Stacking the parameters</h3> <p>The computation made by a layer is given by \(h_{l+1}=f_{l}\left(W_{l}h_{l}+b_{l}\right)=f_{l}\left(a_{l}\right)\). The parameters for this layer are a matrix \(W_{l}\) and a vector\(b_{l}\). But in order to write a concise expression for \(G_{l}\) we need to stack them into a vector \(\theta_{l}\) so that the gradient \(\nabla_{\theta_{l}}\mathcal{L}\) is a vector and writing \(G_{l}^{-1}\nabla_{\theta_{l}}\mathcal{L}\) makes sense. To this end, we use the operator \(vec\) that stacks the column of a matrix into a vector, i.e. for a \(2\times2\) matrix:</p> \[\begin{align*} A= &amp; \left(\begin{array}{cc} A_{11} &amp; A_{12}\\ A_{21} &amp; A_{22} \end{array}\right) &amp; vec\left(A\right)= &amp; \left(\begin{array}{c} A_{11}\\ A_{21}\\ A_{12}\\ A_{22} \end{array}\right) \end{align*}\] <p>Our full vector of parameters becomes:</p> \[\begin{eqnarray*} \theta_{l} &amp; = &amp; \left(\begin{array}{c} vec\left(W\right)\\ b \end{array}\right) \end{eqnarray*}\] <h3 id="expressions-for-the-jacobians-">Expressions for the jacobians<a class="Label" name="subsec:Expressions-for-the"> </a> </h3> <p>We now focus on the block \(G_{l}=\mathbb{E}\left[J_{l}^{T}DJ_{l}\right]\) for layer \(l\). We require an expression for \(J_{l}=\frac{\partial f\left(x;\theta\right)}{\partial\theta_{l}}\). By the chain rule we separate it into a back propagated contribution \(J_{a_{l}}\) and a local contribution:</p> \[\begin{eqnarray*} J_{l} &amp; = &amp; \frac{\partial f\left(x;\theta\right)}{\partial a_{l}}\frac{\partial a_{l}}{\partial\theta_{l}}\\ &amp; = &amp; J_{a_{l}}\frac{\partial a_{l}}{\partial\theta_{l}} \end{eqnarray*}\] <p>To obtain an exact expression for \(\frac{\partial a_{l}}{\partial\theta_{l}}\) we will use \(vec\) once again with the property that \(vec\left(AXB\right)=\left(B^{T}\otimes A\right)vec\left(X\right)\) where \(\otimes\) is the Kronecker product:</p> \[\begin{eqnarray} a_{l} &amp; = &amp; W_{l}h_{l}+b_{l}\nonumber \\ &amp; = &amp; vec\left(W_{l}h_{l}\right)+b_{l}\nonumber \\ &amp; = &amp; vec\left(\mathbf{I}W_{l}h_{l}\right)+b_{l}\\ &amp; = &amp; \left(h_{l}^{T}\otimes\mathbf{I}\right)vec\left(W_{l}\right)+b_{l}\label{eq:flattened_linear} \end{eqnarray}\] <p>In the second line we used the fact that \(W_{l}h_{l}\) is a vector and thus \(vec\left(W_{l}h_{l}\right)=W_{l}h_{l}\). We also introduced \(\mathbf{I}\) the identity matrix of the same size of \(a_{l}\). From eq <a class="Reference" href="#eq:flattened_linear">\ref{eq:flattened_linear}</a> we can directly read the jacobians:</p> \[\begin{eqnarray*} \frac{\partial a_{l}}{\partial vec\left(W_{l}\right)} &amp; = &amp; \left(h_{l}^{T}\otimes\mathbf{I}\right)\\ \frac{\partial a_{l}}{\partial b_{l}} &amp; = &amp; \mathbf{I} \end{eqnarray*}\] <p>Now using \(\theta_{l}\):</p> \[\begin{eqnarray*} \frac{\partial a_{l}}{\partial\theta_{l}} &amp; = &amp; \left(\left(\begin{array}{cc} h_{l}^{T} &amp; 1\end{array}\right)\otimes\mathbf{I}\right) \end{eqnarray*}\] <h3 id="expression-for-the-block">Expression for the block</h3> <p>Getting back to the block \(G_{l}=\mathbb{E}\left[J_{l}^{T}DJ_{l}\right]\) we get a simple expression:</p> \[\begin{eqnarray} G_{l} &amp; = &amp; \mathbb{E}\left[\left(\frac{\partial a_{l}}{\partial\theta_{l}}\right)^{T}J_{a_{l}}^{T}DJ_{a_{l}}\frac{\partial a_{l}}{\partial\theta_{l}}\right]\nonumber \\ &amp; = &amp; \mathbb{E}\left[\left(\left(\begin{array}{cc} h_{l}^{T} &amp; 1\end{array}\right)\otimes\mathbf{I}\right)^{T}J_{a_{l}}^{T}DJ_{a_{l}}\left(\left(\begin{array}{cc} h_{l}^{T} &amp; 1\end{array}\right)\otimes\mathbf{I}\right)\right]\nonumber \\ &amp; = &amp; \mathbb{E}\left[\left(\left(\begin{array}{c} h_{l}\\ 1 \end{array}\right)\otimes\mathbf{I}\right)\left(1\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right)\left(\left(\begin{array}{cc} h_{l}^{T} &amp; 1\end{array}\right)\otimes\mathbf{I}\right)\right]\label{eq:befsim}\\ &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{c} h_{l}\\ 1 \end{array}\right)\left(\begin{array}{cc} h_{l}^{T} &amp; 1\end{array}\right)\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right]\label{eq:aftsim}\\ &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc} h_{l}h_{l}^{T} &amp; h_{l}\\ h_{l}^{T} &amp; 1 \end{array}\right)\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right]\nonumber \end{eqnarray}\] <p>In eq <a class="Reference" href="#eq:befsim">\ref{eq:befsim}</a> we added \(1\otimes\) as it does not change anything. Between eq <a class="Reference" href="#eq:befsim">\ref{eq:befsim}</a> and <a class="Reference" href="#eq:aftsim">\ref{eq:aftsim}</a> we used the fact that \(\left(A\otimes B\right)\left(C\otimes D\right)=AC\otimes BD\) when \(A,B,C,D\) have corresponding sizes (i.e. the products \(AC\) and \(BD\) make sense).</p> <h3 id="discussion">Discussion</h3> <p>We obtained an <i>exact</i> expression for the block corresponding to layer \(l\):</p> \[\begin{eqnarray} G_{l} &amp; = &amp; \mathbb{E}\left[\underbrace{\left(\begin{array}{cc} h_{l}h_{l}^{T} &amp; h_{l}\\ h_{l}^{T} &amp; 1 \end{array}\right)}_{\left(1\right)}\otimes\underbrace{J_{a_{l}}^{T}DJ_{a_{l}}}_{\left(2\right)}\right]\label{eq:exact}\\ &amp; = &amp; \left(\begin{array}{cc} \mathbb{E}\left[h_{l}h_{l}^{T}\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right] &amp; \mathbb{E}\left[h_{l}\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right]\\ \mathbb{E}\left[h_{l}^{T}\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right] &amp; \mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right] \end{array}\right) \end{eqnarray}\] <p>It is an expectation of Kronecker products. Note that we can not swap the expectation and the Kronecker products, and thus while the expression in eq <a class="Reference" href="#eq:exact">\ref{eq:exact}</a> is exact, the one used in KFAC <span class="bibcites">[<a class="bibliocite" name="cite-2" href="#biblio-2"><span class="bib-index">2</span></a>]</span> is an approximation.</p> <p>In eq <a class="Reference" href="#eq:exact">\ref{eq:exact}</a> we denoted by \(\left(2\right)\) the contribution that is backpropagated, and by \(\left(1\right)\) a contribution that is local to the parameters of the layer.</p> <h2 id="inverting-the-matrix">Inverting the matrix</h2> <h3 id="kfac-drill-down">KFAC drill-down</h3> <p>Exactly inverting this matrix can still be untractable for typical neural networks. An approximation that is easier to manipulate is proposed in KFAC <span class="bibcites">[<a class="bibliocite" name="cite-2" href="#biblio-2"><span class="bib-index">2</span></a>]</span>. The key property that we are after here is that for 2 invertible matrices \(A\) and \(B\) we have that \(\left(A\otimes B\right)^{-1}=A^{-1}\otimes B^{-1}\). It becomes:</p> \[\begin{eqnarray*} G_{l} &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc} h_{l}h_{l}^{T} &amp; h_{l}\\ h_{l}^{T} &amp; 1 \end{array}\right)\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right]\\ &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc} h_{l}h_{l}^{T} &amp; h_{l}\\ h_{l}^{T} &amp; 1 \end{array}\right)\right]\otimes\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]+R\\ &amp; \approx &amp; \mathbb{E}\left[\left(\begin{array}{cc} h_{l}h_{l}^{T} &amp; h_{l}\\ h_{l}^{T} &amp; 1 \end{array}\right)\right]\otimes\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right] \end{eqnarray*}\] <p>The residual \(R\) resembles a covariance between both terms:</p> \[\begin{eqnarray*} R &amp; = &amp; \mathbb{E}\left[\left(\left(\begin{array}{cc} h_{l}h_{l}^{T} &amp; h_{l}\\ h_{l}^{T} &amp; 1 \end{array}\right)-\mathbb{E}\left[\left(\begin{array}{cc} h_{l}h_{l}^{T} &amp; h_{l}\\ h_{l}^{T} &amp; 1 \end{array}\right)\right]\right)\otimes\left(J_{a_{l}}^{T}DJ_{a_{l}}-\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]\right)\right]\\ &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc} h_{l}h_{l}^{T}-\mathbb{E}\left[h_{l}h_{l}^{T}\right] &amp; h_{l}-\mathbb{E}\left[h_{l}\right]\\ h_{l}^{T}-\mathbb{E}\left[h_{l}\right] &amp; 0 \end{array}\right)\otimes\left(J_{a_{l}}^{T}DJ_{a_{l}}-\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]\right)\right] \end{eqnarray*}\] <p>The conditions under which it is negligible have not been extensively studied, or at least published to the best of our knowledge. We can however remark that if one part is close to \(0\) then the expected product will be small. This is achieved for instance if \(\left(J_{a_{l}}^{T}DJ_{a_{l}}-\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]\right)\) is small for all \(x\sim p\left(x\right)\) the data generating distribution (recall that \(D\) and \(J_{a_{l}}\) depend on \(x\)). To put it into words if the value of \(J_{a_{l}}^{T}DJ_{a_{l}}\) does not vary much for all training examples. By symmetry we can make a similar argument for \(\left(h_{l}h_{l}^{T}-\mathbb{E}\left[h_{l}h_{l}^{T}\right]\right)\).</p> <h3 id="kfac-inversion-">KFAC inversion<a class="Label" name="subsec:KFAC-inversion"> </a> </h3> <p>We now have a factorized approximate expression for \(G_{l}\):</p> \[\begin{eqnarray*} G_{l}^{\text{approx}} &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc} h_{l}h_{l}^{T} &amp; h_{l}\\ h_{l}^{T} &amp; 1 \end{array}\right)\right]\otimes\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]\\ &amp; = &amp; \left(\begin{array}{cc} \mathbb{E}\left[h_{l}h_{l}^{T}\right] &amp; \mathbb{E}\left[h_{l}\right]\\ \mathbb{E}\left[h_{l}^{T}\right] &amp; 1 \end{array}\right)\otimes\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]\\ &amp; = &amp; A\otimes B \end{eqnarray*}\] <p>Note that while the derivations proposed in KFAC use a single vector \(\theta\) for all parameters of the layer \(l\), we explicitely separated the weight matrix \(W\) and the bias \(b\) in section <a class="Reference" href="#subsec:Expressions-for-the">2.2↑</a> which gives a slightly different expression. Thus the matrix \(A\) is separated into 2 blocks: 2 blocks on the diagonal that correspond to the weight matrix (block \(1,1\)) and the bias (block \(2,2\)), and 2 cross-terms that explicit their interactions.</p> <p>We will see that separating the bias gives a nicer interpretation with a covariance matrix (as opposed to non-centered statistics).</p> <p>We can now use the property \(\left(G_{l}^{\text{approx}}\right)^{-1}=A^{-1}\otimes B^{-1}\). \(B^{-1}\) can not be be further simplified, so the next part is to obtain an expression for \(A^{-1}\):</p> \[\begin{eqnarray*} A^{-1} &amp; = &amp; \left(\begin{array}{cc} \mathbb{E}\left[h_{l}h_{l}^{T}\right] &amp; \mathbb{E}\left[h_{l}\right]\\ \mathbb{E}\left[h_{l}^{T}\right] &amp; 1 \end{array}\right)^{-1} \end{eqnarray*}\] <p>We can use the formula for inverting a block matrix (see <a class="URL" href="https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_inversion" rel="external nofollow noopener" target="_blank">Wikipedia:Block Matrix</a>). We denote by \(C=\mathbb{E}\left[h_{l}h_{l}^{T}\right]-\mathbb{E}\left[h_{l}\right]\mathbb{E}\left[h_{l}^{T}\right]\) and we get:</p> \[\begin{eqnarray*} A^{-1} &amp; = &amp; \left(\begin{array}{cc} C^{-1} &amp; -C^{-1}\mathbb{E}\left[h_{l}\right]\\ -\mathbb{E}\left[h_{l}^{T}\right]C^{-1} &amp; 1+\mathbb{E}\left[h_{l}^{T}\right]C^{-1}\mathbb{E}\left[h_{l}\right] \end{array}\right) \end{eqnarray*}\] <p>Note that \(C\) is the covariance of \(h_{l}\): \(C=\mathbb{E}\left[\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)^{T}\right]=cov\left(h_{l}\right)\). It is centered (we substract \(\mathbb{E}\left[h_{l}\right]\)) which follows from the block matrix inversion formula, which in turns follows from the fact that we separated the bias. This motivates the use of centered statistics in second order inspired algorithms.</p> <h2 id="writing-the-update">Writing the update</h2> <h3 id="derivation">Derivation</h3> <p>Now that we have an expression for \(G_{l}^{-1}\) we can write the product \(G_{l}^{-1}\nabla_{\theta_{l}}\mathcal{L}\) required to make an update \(\theta_{l}^{t+1}=\theta_{l}^{t}-\lambda G_{l}^{-1}\nabla_{\theta_{l}}\mathcal{L}\). In section <a class="Reference" href="#subsec:Expressions-for-the">2.2↑</a> we wrote an expression for the jacobians \(J_{l}=\frac{\partial f\left(x;\theta\right)}{\partial\theta_{l}}\). By a similar analysis we can write the gradients \(\nabla_{\theta_{l}}\mathcal{L}\):</p> \[\begin{eqnarray*} \nabla_{\theta_{l}}\mathcal{L} &amp; = &amp; \mathbb{E}\left[\nabla_{\theta_{l}}\ell\left(x\right)\right]\\ &amp; = &amp; \mathbb{E}\left[\left(\frac{\partial\ell\left(x\right)}{\partial\theta_{l}}\right)^{T}\right]\\ &amp; = &amp; \mathbb{E}\left[\left(\frac{\partial\ell\left(x\right)}{\partial a_{l}}\frac{\partial a_{l}}{\partial\theta_{l}}\right)^{T}\right]\\ &amp; = &amp; \mathbb{E}\left[\left(\frac{\partial a_{l}}{\partial\theta_{l}}\right)^{T}\left(\frac{\partial\ell\left(x\right)}{\partial a_{l}}\right)^{T}\right]\\ &amp; = &amp; \mathbb{E}\left[\left(\frac{\partial a_{l}}{\partial\theta_{l}}\right)^{T}\nabla_{a_{l}}\ell\left(x\right)\right] \end{eqnarray*}\] <p>Using the same expressions as in <a class="Reference" href="#subsec:Expressions-for-the">2.2↑</a> we can simplify \(\frac{\partial a_{l}}{\partial\theta_{l}}=\left(\left(\begin{array}{cc} h_{l}^{T} &amp; 1\end{array}\right)\otimes\mathbf{I}\right)\):</p> \[\begin{eqnarray*} \nabla_{\theta_{l}}\mathcal{L} &amp; = &amp; \mathbb{E}\left[\left(\left(\begin{array}{c} h_{l}\\ 1 \end{array}\right)\otimes\mathbf{I}\right)\nabla_{a_{l}}\ell\left(x\right)\right]\\ &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{c} h_{l}\\ 1 \end{array}\right)\otimes\nabla_{a_{l}}\ell\left(x\right)\right] \end{eqnarray*}\] <p>Multiplying together with \(\left(G_{l}^{\text{approx}}\right)^{-1}\) we get the product \(\Delta_{\theta_{l}}=\left(G_{l}^{\text{approx}}\right)^{-1}\nabla_{\theta_{l}}\mathcal{L}\):</p> \[\begin{eqnarray*} \Delta_{\theta_{l}} &amp; = &amp; \left(A^{-1}\otimes B^{-1}\right)\nabla_{\theta_{l}}\mathcal{L}\\ &amp; = &amp; \left(\left(\begin{array}{cc} C^{-1} &amp; -C^{-1}\mathbb{E}\left[h_{l}\right]\\ -\mathbb{E}\left[h_{l}^{T}\right]C^{-1} &amp; 1+\mathbb{E}\left[h_{l}^{T}\right]C^{-1}\mathbb{E}\left[h_{l}\right] \end{array}\right)\otimes B^{-1}\right)\mathbb{E}\left[\left(\begin{array}{c} h_{l}\\ 1 \end{array}\right)\otimes\nabla_{a_{l}}\ell\left(x\right)\right]\\ &amp; = &amp; \left(\begin{array}{c} C^{-1}\mathbb{E}\left[\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\otimes B^{-1}\nabla_{a_{l}}\ell\left(x\right)\right]\\ \mathbb{E}\left[\left(1-\mathbb{E}\left[h_{l}^{T}\right]C^{-1}\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\right)B^{-1}\nabla_{a_{l}}\ell\left(x\right)\right] \end{array}\right) \end{eqnarray*}\] <p>In the first line we can read the update for \(W\) (in fact its vectorized version \(vec\left(W\right)\)), and the second line is the update for \(b\).</p> <h3 id="updating-the-weight-matrix-w-">Updating the weight matrix \(W\)<a class="Label" name="subsec:Updating-the-weight"> </a> </h3> <p>The new update for \(W\) is given by:</p> \[\begin{eqnarray*} \Delta_{\text{vec}\left(W_{l}\right)} &amp; = &amp; C^{-1}\mathbb{E}\left[\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\otimes B^{-1}\nabla_{a_{l}}\ell\left(x\right)\right] \end{eqnarray*}\] <p>We some algebraic manipulations we get back to the expression for the unflattened matrix:\(\begin{eqnarray*} \Delta_{\text{vec}\left(W_{l}\right)} &amp; = &amp; \left(C^{-1}\otimes B^{-1}\right)\mathbb{E}\left[\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\otimes\nabla_{a_{l}}\ell\left(x\right)\right]\\ &amp; = &amp; \left(C^{-1}\otimes B^{-1}\right)\mathbb{E}\left[\text{vec}\left(\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\otimes\nabla_{a_{l}}\ell\left(x\right)\right)\right]\\ \Delta_{W_{l}} &amp; = &amp; B^{-1}\mathbb{E}\left[\nabla_{a_{l}}\ell\left(x\right)\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)^{T}\right]C^{-1} \end{eqnarray*}\)</p> <p>This is to compare with the usual gradient descent update given by:</p> \[\begin{eqnarray*} \nabla_{W_{l}}\mathcal{L} &amp; = &amp; \mathbb{E}\left[\nabla_{a_{l}}\ell\left(x\right)h_{l}^{T}\right] \end{eqnarray*}\] <p>We can notice 2 additions:</p> <ul> <li> the update is rescaled and rotated using the 2 matrices \(B^{-1}\) and \(C^{-1}\) </li> <li> the expectation is centered by substracting \(\mathbb{E}\left[h_{l}\right]\) </li> </ul> <p>In addition to the derivation proposed in KFAC, by expliciting the bias we obtained 2 different centerings:</p> <ul> <li> the covariance matrix \(C\) </li> <li> the expectation is centered by substracting \(\mathbb{E}\left[h_{l}\right]\) </li> </ul> <p>### Updating the bias vector \(b\)</p> <p>The new update for \(b\) is given by:</p> \[\begin{eqnarray*} \Delta_{b_{l}} &amp; = &amp; \mathbb{E}\left[\left(1-\mathbb{E}\left[h_{l}^{T}\right]C^{-1}\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\right)B^{-1}\nabla_{a_{l}}\ell\left(x\right)\right] \end{eqnarray*}\] <p>This is to compare with the usual gradient descent update given by:</p> \[\begin{eqnarray*} \nabla_{b_{l}}\mathcal{L} &amp; = &amp; \mathbb{E}\left[\nabla_{a_{l}}\ell\left(x\right)\right] \end{eqnarray*}\] <p>Once again we notice that the update is scaled and rotated using \(B^{-1}\) but there is also this strange scalar scaling \(\left(1-\mathbb{E}\left[h_{l}^{T}\right]C^{-1}\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\right)\) for which we are not able to give an interpretation. However in practice we noted that we did not have any performance gain compared to using only \(1\).</p> <h2 id="conclusion">Conclusion</h2> <p>We gave explicit derivation of the second order updates used in Gauss-Newton and in Natural gradient. By explicitly separating the weight matrices and the bias we obtained a nice centering term in both the covariance matrix used to rotate the update \(C=\mathbb{E}\left[\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)^{T}\right]\), and in the expectation used to compute the gradient \(\mathbb{E}\left[\nabla_{a_{l}}\ell\left(x\right)\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)^{T}\right]\).</p> <p>It is well-known that centering things is often useful. It is sometimes referred as the centering trick, or mean only batch norm. Another efficient technique called natural neural networks <span class="bibcites">[<a class="bibliocite" name="cite-1" href="#biblio-1"><span class="bib-index">1</span></a>]</span> building on the structure of the FIM mentions the trick without giving it much justification. To the best that we know this justification based on the structure of the second order matrices has not yet been contributed. We hope that this blog note can enlighten deep learning practitioners who are not very familliar with second order methods, in order to invent new approximate algorithms with more efficient updates.</p> <h2 id="references">References</h2> <p class="biblio"> <span class="entry">[<a class="biblioentry" name="biblio-1"><span class="bib-index">1</span></a>] </span> <span class="bib-authors">Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, others</span>. <span class="bib-title">Natural neural networks</span>. <i><span class="bib-booktitle">Advances in Neural Information Processing Systems</span></i>:<span class="bib-pages">2071—2079</span>, <span class="bib-year">2015</span>. </p> <p class="biblio"> <span class="entry">[<a class="biblioentry" name="biblio-2"><span class="bib-index">2</span></a>] </span> <span class="bib-authors">James Martens, Roger Grosse</span>. <span class="bib-title">Optimizing neural networks with kronecker-factored approximate curvature</span>. <i><span class="bib-booktitle">International Conference on Machine Learning</span></i>:<span class="bib-pages">2408—2417</span>, <span class="bib-year">2015</span>. </p> <p class="biblio"> <span class="entry">[<a class="biblioentry" name="biblio-3"><span class="bib-index">3</span></a>] </span> <span class="bib-authors">Razvan Pascanu, Yoshua Bengio</span>. <span class="bib-title">Revisiting natural gradient for deep networks</span>. <i><span class="bib-journal">arXiv preprint arXiv:1301.3584</span></i>, <span class="bib-year">2013</span>. </p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/batchnorm-derivatives/">Derivatives through a batch norm layer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/empirical-fisher/">What is the empirical Fisher ?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/fisher-conditional/">How to compute the Fisher of a conditional when applying natural gradient to neural networks?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/demystifying-natural-nn/">Demystifying Natural Neural Networks</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Thomas George. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-99149577-1"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-99149577-1");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications and pre-prints, also see my google scholar profile",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-software",title:"software",description:"Pieces of code I developed during my research and that I am releasing open-source. For a complete list go to my github page.",section:"Navigation",handler:()=>{window.location.href="/software/"}},{id:"post-derivatives-through-a-batch-norm-layer",title:"Derivatives through a batch norm layer",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/batchnorm-derivatives/"}},{id:"post-what-is-the-empirical-fisher",title:"What is the empirical Fisher ?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/empirical-fisher/"}},{id:"post-how-to-compute-the-fisher-of-a-conditional-when-applying-natural-gradient-to-neural-networks",title:"How to compute the Fisher of a conditional when applying natural gradient to...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/fisher-conditional/"}},{id:"post-the-algebra-of-second-order-methods-in-neural-networks",title:"The algebra of second order methods in neural networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/algebra2ndorder/"}},{id:"post-demystifying-natural-neural-networks",title:"Demystifying Natural Neural Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/demystifying-natural-nn/"}},{id:"news-i-will-give-a-talk-titled-optimization-and-generalization-through-the-lens-of-the-linearization-of-neural-networks-training-dynamics-in-front-of-roger-grosse-https-www-cs-toronto-edu-rgrosse-39-s-group-at-vector-institute-toronto",title:"I will give a talk titled _Optimization and generalization through the lens of...",description:"",section:"News"},{id:"news-i-will-present-nngeometry-https-github-com-tfjgeorge-nngeometry-at-the-pytorch-ecosystem-day-https-pytorch-org-ecosystem-pted-2021",title:"I will present [NNGeometry](https://github.com/tfjgeorge/nngeometry/) at the [Pytorch Ecosystem Day](https://pytorch.org/ecosystem/pted/2021)",description:"",section:"News"},{id:"news-presentation-of-implicit-regularization-via-neural-feature-alignment-at-conf\xe9rence-sur-l-39-apprentissage-automatique-2021-https-cap2021-sciencesconf-org-resource-page-id-8-html",title:"Presentation of _Implicit Regularization via Neural Feature Alignment_ at [Conf\xe9rence sur l&#39;Apprentissage Automatique...",description:"",section:"News"},{id:"news-presentation-of-our-workshop-paper-continual-learning-and-deep-networks-an-analysis-of-the-last-layer-with-timoth\xe9e-lesort-at-the-theory-of-continual-learning-workshop-at-icml",title:"Presentation of our workshop paper *Continual learning and Deep Networks: an Analysis of...",description:"",section:"News"},{id:"news-i-will-present-our-recent-work-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-paper-assets-pdf-lazy-vs-hasty-icml-scis-2022-pdf-poster-assets-pdf-lazy-vs-hasty-icml-scis-2022-poster-pdf-at-the-scis-https-sites-google-com-view-scis-workshop-home-workshop-at-icml-2022",title:"I will present our recent work _Lazy vs hasty: linearization in deep networks...",description:"",section:"News"},{id:"news-new-pre-print-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-https-arxiv-org-abs-2209-09658-available",title:"New pre-print [Lazy vs hasty: linearization in deep networks impacts learning schedule based...",description:"",section:"News"},{id:"news-our-paper-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-https-openreview-net-pdf-id-lukvf4vrfp-was-accepted-to-tmlr-https-openreview-net-forum-id-lukvf4vrfp-code-https-github-com-tfjgeorge-lazy-vs-hasty",title:"Our paper [Lazy vs hasty: linearization in deep networks impacts learning schedule based...",description:"",section:"News"},{id:"news-i-successfully-defended-my-phd-thesis-deep-networks-training-and-generalization-insights-from-linearization-manuscript-assets-pdf-thomas-george-phd-thesis-pdf-slides-assets-pdf-phd-presentation-thomas-george-pdf",title:"I successfully defended my PhD Thesis: **Deep networks training and generalization: insights from...",description:"",section:"News"},{id:"news-i-am-now-a-postdoctoral-researcher-at-orange-labs-https-hellofuture-orange-com-fr-in-vincent-lemaire-http-vincentlemaire-labs-fr-39-s-group",title:"I am now a postdoctoral researcher at [Orange Labs](https://hellofuture.orange.com/fr/) in [Vincent Lemaire](http://vincentlemaire-labs.fr/)&#39;s group....",description:"",section:"News"},{id:"news-i-will-present-our-paper-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-https-openreview-net-pdf-id-lukvf4vrfp-at-conf\xe9rence-sur-l-39-apprentissage-automatique-cap-https-pfia23-icube-unistra-fr-conferences-cap-index-html-in-strasbourg",title:"I will present our paper [Lazy vs hasty: linearization in deep networks impacts...",description:"",section:"News"},{id:"news-i-joined-orange-innovation-https-hellofuture-orange-com-fr-as-a-permanent-research-scientist-in-the-responsible-and-impactful-ai-program",title:"I joined [Orange Innovation](https://hellofuture.orange.com/fr/) as a permanent research scientist in the Responsible and...",description:"",section:"News"},{id:"news-our-paper-mislabeled-examples-detection-viewed-as-probing-machine-learning-models-concepts-survey-and-extensive-benchmark-https-openreview-net-pdf-id-3ylor7bhkx-was-accepted-to-tmlr-https-openreview-net-forum-id-3ylor7bhkx-code-https-github-com-orange-opensource-mislabeled-video-https-youtu-be-ft9vzxs0nh8",title:"Our paper [Mislabeled examples detection viewed as probing machine learning models: concepts, survey...",description:"",section:"News"},{id:"news-together-with-pierre-nodet-https-scholar-google-com-citations-user-ido8qgeaaaaj-we-will-present-our-recent-works-on-weakly-supervised-learning-at-the-lfi-lip6-seminary-https-lfi-lip6-fr-seminaires",title:"Together with [Pierre Nodet](https://scholar.google.com/citations?user=ido8qGEAAAAJ), we will present our recent works on Weakly supervised...",description:"",section:"News"},{id:"news-i-will-attend-egc-https-www-egc2025-cnrs-fr-in-strasbourg-to-present-our-paper-calibration-des-mod\xe8les-d-apprentissage-pour-l-am\xe9lioration-des-d\xe9tecteurs-automatiques-d-exemples-mal-\xe9tiquet\xe9s-assets-pdf-mislabeled-calibration-paper-fr-pdf-as-well-as-the-workshop-iacd-https-sites-google-com-view-atelier-iacd-2025-for-a-presentation-of-our-paper-mislabeled-examples-detection-viewed-as-probing-machine-learning-models-concepts-survey-and-extensive-benchmark-https-openreview-net-pdf-id-3ylor7bhkx",title:"I will attend [EGC](https://www.egc2025.cnrs.fr/) in Strasbourg to present our paper [Calibration des mod\xe8les...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%74%66%6A%67%65%6F%72%67%65@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=pc3_ujYAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/tfjgeorge","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/tfjgeorge","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/https://bsky.app/profile/tfjgeorge.bsky.social","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>