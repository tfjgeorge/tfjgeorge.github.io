<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Demystifying Natural Neural Networks | Thomas  George</title>
    <meta name="author" content="Thomas  George">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tfjgeorge.github.io/blog/2017/demystifying-natural-nn/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Thomas </span>George</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/software/">software</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Demystifying Natural Neural Networks</h1>
    <p class="post-meta">May 23, 2017</p>
    <p class="post-tags">
      <a href="/blog/2017"> <i class="fas fa-calendar fa-sm"></i> 2017 </a>
        ·  
        <a href="/blog/category/note">
          <i class="fas fa-tag fa-sm"></i> note</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <!--starthtml-->

<p><span class="bibcites">[<a class="bibliocite" name="cite-1" href="#biblio-1"><span class="bib-index">1</span></a>]</span> introduced a reparametrization for feedforward neural networks that gives a huge improvement in performance for optimizing neural networks. In this blog note I will summarize the main idea and introduce a new notation that clarifies the centering trick used in the paper.</p>

<h2 id="the-fim-in-the-context-of-the-natural-gradient">The FIM in the context of the Natural Gradient</h2>

<h3 id="fisher-information-matrix">Fisher Information Matrix</h3>

<p>The Fisher information matrix (FIM) is a tool well used in statistics. In the context of machine learning, and in particular deep learning, we use its inverse as a preconditioner for the gradient descent algorithm. In this section, we show how the FIM can be derived from the KL divergence and how we get a better “natural” gradient using this information. Let us first recall the definition of the KL divergence for 2 distributions \(p\) and \(q\):</p>

\[\begin{eqnarray*}
\text{KL}\left(p\parallel q\right) &amp; = &amp; \mathbb{E}_{p}\left[\log\left(\frac{p}{q}\right)\right]
\end{eqnarray*}\]

<p>It is a non-negative quantity that looks like a measure of how much \(q\) differs from \(p\). In particular, \(\text{KL}\left(p\parallel q\right)=0\) when \(p=q\). Note that it is not symmetric, so it can not directly be used as a metric.</p>

<p>The idea of the natural gradient is to use the KL divergence as a regularizer when doing gradient descent. We will denote by \(p_{\theta}\) a parametric model and \(\Delta\theta\) a change in its parameter values. \(\text{KL}\left(p_{\theta}\parallel p_{\theta+\Delta\theta}\right)\) is used as our regularizer, so that each change \(\Delta\theta\) gives the same change in the distribution space. Instead of using the full expression for \(\text{KL}\left(p_{\theta}\parallel p_{\theta+\Delta\theta}\right)\) we will use its second order Taylor series around \(\theta\) (for full derivation see for instance <span class="bibcites">[<a class="bibliocite" name="cite-3" href="#biblio-3"><span class="bib-index">3</span></a>]</span>):</p>

\[\begin{eqnarray*}
\text{KL}\left(p_{\theta}\Vert p_{\theta+\Delta\theta}\right) &amp; = &amp; \Delta\theta^{T}\mathbf{F}\Delta\theta+o(\left\Vert \Delta\theta\right\Vert _{2}^{2})
\end{eqnarray*}\]

<p>This expression exhibits the FIM which can now be used directly as a regularizer.</p>

<h3 id="natural-gradient-descent">(Natural) gradient descent</h3>

<p>The usual gradient descent algorithm can be formulated as the minimization of the following expression:</p>

\[\begin{eqnarray*}
\Delta\theta &amp; = &amp; \text{argmin}_{\Delta\theta}\left\{ \Delta\theta^{T}\nabla\mathcal{L}+\frac{\epsilon}{2}\left\Vert \Delta\theta\right\Vert ^{2}\right\}
\end{eqnarray*}\]

<p>We use the notation \(\mathcal{L}\) for the expectation of the loss function over the distribution of the data. This expression can be easily solved giving the expression \(\Delta\theta=-\frac{1}{\lambda}\nabla\mathcal{L}\). The parameter \(\epsilon\) is the inverse of the learning rate, and controls how much each parameter can change. We will now add a new regularizer using the FIM, and transform the minimization problem into:</p>

\[\begin{eqnarray*}
\Delta\theta &amp; = &amp; \text{argmin}_{\Delta\theta}\left\{ \Delta\theta^{T}\nabla\mathcal{L}+\frac{\epsilon}{2}\left\Vert \Delta\theta\right\Vert ^{2}+\frac{\lambda}{2}\Delta\theta^{T}\mathbf{F}\Delta\theta\right\}
\end{eqnarray*}\]

<p>We now constrain our gradient step to be small in term of change of parameter values, and also to be small in term of how much the resulting distribution changes. This expression can be solved to give \(\Delta\theta=\frac{1}{\lambda}\left(\mathbf{F}+\epsilon\mathbf{I}\right)^{-1}\nabla\mathcal{L}\). This expression also gives an insight for the role of \(\lambda\) and \(\epsilon\), which control 2 different but related quantities expressed by our constraints. This new update is called the natural gradient.</p>

<h2 id="factorizing-the-fim-for-neural-networks">Factorizing the FIM for neural networks</h2>

<h3 id="an-expression-for-the-fim-using-jacobians">An expression for the FIM using jacobians</h3>

<p>Interestingly, for the usual distributions expressed by neural networks, the FIM takes the following simple form as shown by <span class="bibcites">[<a class="bibliocite" name="cite-3" href="#biblio-3"><span class="bib-index">3</span></a>]</span>:</p>

\[\begin{eqnarray*}
\mathbf{F} &amp; = &amp; \mathbb{E}_{x\sim q}\left[\boldsymbol{J}_{\boldsymbol{y}\left(x\right)}^{T}D\left(\boldsymbol{y}\left(x\right)\right)\boldsymbol{J}_{\boldsymbol{y}\left(x\right)}\right]
\end{eqnarray*}\]

<p>The values for \(x\) are drawn from the data generating distribution \(q\). The notation \(\boldsymbol{J}_{\boldsymbol{y}\left(x\right)}\) is used for the jacobian of the output of the network (i.e. the probability expressed at a given \(x\) : \(p\left(y\mid x\right)\)), with respect to the parameters. In other words, it measures how much the output of the network will change for a given \(x\) if we change the parameters. \(D\) is a diagonal matrix with non negative diagonal terms, and depends of the cost function used. For the quadratic loss it is the identity.</p>

<h3 id="approximations-to-the-fim">Approximations to the FIM</h3>

<p>The FIM is difficult to compute because of its size (\(n_{parameters}\times n_{parameters}\)) and because in general we do not have an expression for \(q\) but only samples from a training dataset.</p>

<p>A first approximation that we can make is by ignoring the interactions between layers. In this case the FIM takes the form of a block diagonal matrix, where each block is a square matrix which has the size of the parameters of a layer. For a neural network with \(n_{layers}\) layers this reduces the FIM into \(n_{layers}\) smaller matrices. We will denote by \(\mathbf{F}_{i}\) the block corresponding to layer \(i\).</p>

<p>A second common approximation we make in practice is to use the empirical FIM for a training dataset of \(n\) examples \(x_{i}\): \(\mathbf{F}=\frac{1}{n}\sum_{i}\boldsymbol{J}_{\boldsymbol{y}\left(x_{i}\right)}^{T}D\left(\boldsymbol{y}\left(x_{i}\right)\right)\boldsymbol{J}_{\boldsymbol{y}\left(x_{i}\right)}\).</p>

<h3 id="factorization-of-the-mathbff_is-using-the-kronecker-product">Factorization of the \(\mathbf{F}_{i}\)s using the Kronecker product</h3>

<p>In the rest of this note, we will restrict our analysis to a multilayer perceptron. Each layer is parametrized using a weight matrix \(W\) of size \(\left(\text{out}\times\text{in}\right)\) and a bias vector \(b\) of size \(\left(\text{out}\right)\). A layer consists in a linear transformation and a non-linearity \(f\) to give the hidden representation of the next layer:</p>

\[\begin{eqnarray*}
h_{l+1}=f_{l}\left(a_{l}\right) &amp; \text{with} &amp; a_{l}=W_{l}h_{l}+b_{l}
\end{eqnarray*}\]

<p>We will focus on a single layer, and drop the subscript \(l\). In the following, we will also focus on a single example in the expectation for the FIM. Each individual example has its own jacobian with respect to the parameter. In other words, this jacobian measures how the output of the network changes for this example, if you move the parameter values.</p>

<p>The jacobians for parameters \(W\) and \(b\) are obtained using the chain rule for derivation: \(\boldsymbol{J}_{\boldsymbol{y}}^{W}=\boldsymbol{J}_{\boldsymbol{y}}^{a}\boldsymbol{J}_{a}^{W}\) and \(\boldsymbol{J}_{\boldsymbol{y}}^{b}=\boldsymbol{J}_{\boldsymbol{y}}^{a}\boldsymbol{J}_{a}^{b}\). The jacobian for the bias simplifies as \(\boldsymbol{J}_{a}^{b}=\mathbf{I}\). For the weight matrix, it is a little bit more tricky. As \(W\) is a matrix and not a vector, we can not express a jacobian matrix directly. We will have to make use of the Kronecker product and the \(\text{vec}\) operator. We start from the linear relation \(a=Wh+b\) and remark that \(a=\text{vec}\left(a\right)\) since \(a\) is a vector. We can now make use of the formula \(\text{vec}\left(AXB\right)=\left(B^{T}\otimes A\right)\text{vec}\left(X\right)\) to obtain:</p>

\[\begin{eqnarray*}
a &amp; = &amp; \left(h^{T}\otimes\mathbf{I}_{out}\right)\text{vec}\left(W\right)\\
\boldsymbol{J}_{a}^{\text{vec}\left(W\right)} &amp; = &amp; \left(h^{T}\otimes\mathbf{I}_{out}\right)
\end{eqnarray*}\]

<p>Putting everything together we get the jacobians:</p>

\[\begin{eqnarray*}
\boldsymbol{J}_{\boldsymbol{y}}^{\text{vec}\left(W\right)} &amp; = &amp; \boldsymbol{J}_{\boldsymbol{y}}^{a}\left(h^{T}\otimes\mathbf{I}_{out}\right)\\
 &amp; = &amp; h^{T}\otimes\boldsymbol{J}_{\boldsymbol{y}}^{a}\\
\boldsymbol{J}_{\boldsymbol{y}}^{b} &amp; = &amp; \boldsymbol{J}_{\boldsymbol{y}}^{a}
\end{eqnarray*}\]

<p>Now imagine that we stack all parameters \(W\) and \(b\) in a vector \(\theta=\left(\text{vec}\left(W\right)_{1}\cdots\text{vec}\left(W\right)_{in\times out}b_{1}\cdots b_{out}\right)\). We get the full jacobian by stacking the jacobians:</p>

\[\begin{eqnarray*}
\boldsymbol{J}_{\boldsymbol{y}}^{\theta} &amp; = &amp; \left(\begin{array}{cc}
h^{T} &amp; 1\end{array}\right)\otimes\boldsymbol{J}_{\boldsymbol{y}}^{a}
\end{eqnarray*}\]

<p>From this expression we can finally express the FIM in a factorized form:</p>

\[\begin{eqnarray}
\mathbf{F} &amp; = &amp; \mathbb{E}\left[\left\{ \left(\begin{array}{c}
h\\
1
\end{array}\right)\otimes\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}\right\} D\left(\boldsymbol{y}\right)\left\{ \left(\begin{array}{cc}
h^{T} &amp; 1\end{array}\right)\otimes\boldsymbol{J}_{\boldsymbol{y}}^{a}\right\} \right]\nonumber \\
 &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc}
hh^{T} &amp; h\\
h^{T} &amp; 1
\end{array}\right)\otimes\left(\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}D\left(\boldsymbol{y}\right)\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)\right]\label{eq:factoredfim}
\end{eqnarray}\]

<p>In this expression, remember that all 3 variables \(h\), \(a\) and \(\boldsymbol{y}\) have a different value for each individual example. The FIM is the sum of the contributions of each example. The use of the Kronecker product permits splitting the contribution of each example in a term that involves the input of the layer \(\left(\begin{array}{cc}
hh^{T} &amp; h\\
h^{T} &amp; 1
\end{array}\right)\) and a term that involves the jacobian received on the output of the linear transformation \(\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}D\left(\boldsymbol{y}\right)\boldsymbol{J}_{\boldsymbol{y}}^{a}\).</p>

<h2 id="kfac">KFAC</h2>

<p><span class="bibcites">[<a class="bibliocite" name="cite-2" href="#biblio-2"><span class="bib-index">2</span></a>]</span> use a simplification of the FIM that drastically reduce the computation required for inverting it. Remember that even if we consider a block diagonal approximation of the FIM, each block still has size \(n_{parameters}\times n_{parameters}\). We need to invert this block, and the operation of inverting a square matrix is \(O\left(n_{parameters}^{3}\right)\). They propose the following approximation:</p>

\[\begin{eqnarray*}
\mathbf{F} &amp; \approx &amp; \mathbb{E}\left[\left(\begin{array}{cc}
hh^{T} &amp; h\\
h^{T} &amp; 1
\end{array}\right)\right]\otimes\mathbb{E}\left[\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}D\left(\boldsymbol{y}\right)\boldsymbol{J}_{\boldsymbol{y}}^{a}\right]
\end{eqnarray*}\]

<p>The Kronecker product has the nice property that for 2 invertible square matrices \(A\) and \(B\), \(\left(A\otimes B\right)^{-1}=A^{-1}\otimes B^{-1}\). It follows that inverting the FIM now requires inverting 2 smaller matrices. However the approximation they use is questionable, and they show some interesting experimental results.</p>

<h2 id="natural-neural-networks">Natural Neural Networks</h2>

<p><span class="bibcites">[<a class="bibliocite" name="cite-1" href="#biblio-1"><span class="bib-index">1</span></a>]</span> exploit the same factorization <a class="Reference" href="#eq:factoredfim">↓</a> but only consider the term involving the input of the linear transformation (\(h\)). They propose a reparametrization that will make \(\mathbb{E}\left[\left(\begin{array}{cc}
hh^{T} &amp; h\\
h^{T} &amp; 1
\end{array}\right)\right]\) equal the identity. To this view, they change the original linear transformation \(a=Wh+b\) to become:</p>

\[\begin{eqnarray*}
a &amp; = &amp; VU\left(h-\mu\right)+d
\end{eqnarray*}\]

<p>\(V\) is the new weight matrix and \(d\) are the new biases. \(\mu=\mathbb{E}\left[h\right]\) is the mean value for \(h\) and \(U\) is the square root of the inverse covariance of \(h\), defined by \(U^{2}=\left(\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]\right)^{-1}\), denoted by \(U=\left(\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]\right)^{-\frac{1}{2}}\). \(U\) and \(\mu\) are not trained using gradient descent but instead they are estimated using data from the training set.</p>

<p>Our new parameters \(V\) and \(d\) are trained using gradient descent, which will now have the desired property. We will denote by \(h_{e}=U\left(h-\mu\right)\) our new “effective” input to the linear transformation induced by the weight matrix \(V\). Let us first remark that \(\mathbb{E}\left[h_{e}\right]=U\left(\mathbb{E}\left[h\right]-\mu\right)=U\left(\mu-\mu\right)=0\), so the new input is centered on average. A second remark is that \(\mathbb{E}\left[h_{e}h_{e}^{T}\right]=U\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]U^{T}=\mathbf{I}\). By construction \(U\) cancels out the covariance. Wrapping everything together we thus have the desired property that:</p>

\[\begin{eqnarray*}
\mathbb{E}\left[\left(\begin{array}{cc}
h_{e}h_{e}^{T} &amp; h_{e}\\
h_{e}^{T} &amp; 1
\end{array}\right)\right] &amp; = &amp; \left(\begin{array}{cc}
\mathbb{E}\left[h_{e}h_{e}^{T}\right] &amp; \mathbb{E}\left[h_{e}\right]\\
\mathbb{E}\left[h_{e}^{T}\right] &amp; 1
\end{array}\right)\\
 &amp; = &amp; \left(\begin{array}{cc}
\mathbf{I} &amp; 0\\
0 &amp; 1
\end{array}\right)
\end{eqnarray*}\]

<p>The FIM for our new reparametrization thus has a better form, and they also show experimentally that this method is very efficient, and that by amortizing the cost of inverting the covariance matrix over several parameter updates, it can be faster that standard SGD.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this blog note we showed the path from the intuituion behind using the FIM as a preconditioner for gradient descent, to 2 recent algorithms that use the FIM. We also introduced the complete form for the FIM, also including the bias, which in the best of our knowledge has not been explicited in any publication at this time.</p>

<p>We think that the FIM or other similar preconditioners, and their factorizations, will open a new serie of algorithms to optimize neural networks, that will help make progress in artificial intelligence tasks.</p>

<h2 id="references">References</h2>

<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-1"><span class="bib-index">1</span></a>] </span> <span class="bib-authors">Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, others</span>: “<span class="bib-title">Natural neural networks</span>”, <i><span class="bib-booktitle">Advances in Neural Information Processing Systems</span></i>, pp. <span class="bib-pages">2071—2079</span>, <span class="bib-year">2015</span>.

</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-2"><span class="bib-index">2</span></a>] </span> <span class="bib-authors">James Martens, Roger B Grosse</span>: “<span class="bib-title">Optimizing Neural Networks with Kronecker-factored Approximate Curvature.</span>”, <i><span class="bib-booktitle">ICML</span></i>, pp. <span class="bib-pages">2408—2417</span>, <span class="bib-year">2015</span>.

</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-3"><span class="bib-index">3</span></a>] </span> <span class="bib-authors">Razvan Pascanu, Yoshua Bengio</span>: “<span class="bib-title">Revisiting natural gradient for deep networks</span>”, <i><span class="bib-journal">arXiv preprint arXiv:1301.3584</span></i>, <span class="bib-year">2013</span>.





<!--endhtml-->
</p>

    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/batchnorm-derivatives/">Derivatives through a batch norm layer</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/empirical-fisher/">What is the empirical Fisher ?</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/fisher-conditional/">How to compute the Fisher of a conditional when applying natural gradient to neural networks?</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/algebra2ndorder/">The algebra of second order methods in neural networks</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Thomas  George. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-99149577-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-99149577-1');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
