<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Demystifying Natural Neural Networks | Thomas George </title> <meta name="author" content="Thomas George"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tfjgeorge.github.io/blog/2017/demystifying-natural-nn/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Thomas</span> George </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/software/">software </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Demystifying Natural Neural Networks</h1> <p class="post-meta"> Created in May 23, 2017 </p> <p class="post-tags"> <a href="/blog/2017"> <i class="fa-solid fa-calendar fa-sm"></i> 2017 </a>   ·   <a href="/blog/category/note"> <i class="fa-solid fa-tag fa-sm"></i> note</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><span class="bibcites">[<a class="bibliocite" name="cite-1" href="#biblio-1"><span class="bib-index">1</span></a>]</span> introduced a reparametrization for feedforward neural networks that gives a huge improvement in performance for optimizing neural networks. In this blog note I will summarize the main idea and introduce a new notation that clarifies the centering trick used in the paper.</p> <h2 id="the-fim-in-the-context-of-the-natural-gradient">The FIM in the context of the Natural Gradient</h2> <h3 id="fisher-information-matrix">Fisher Information Matrix</h3> <p>The Fisher information matrix (FIM) is a tool well used in statistics. In the context of machine learning, and in particular deep learning, we use its inverse as a preconditioner for the gradient descent algorithm. In this section, we show how the FIM can be derived from the KL divergence and how we get a better “natural” gradient using this information. Let us first recall the definition of the KL divergence for 2 distributions \(p\) and \(q\):</p> \[\begin{eqnarray*} \text{KL}\left(p\parallel q\right) &amp; = &amp; \mathbb{E}_{p}\left[\log\left(\frac{p}{q}\right)\right] \end{eqnarray*}\] <p>It is a non-negative quantity that looks like a measure of how much \(q\) differs from \(p\). In particular, \(\text{KL}\left(p\parallel q\right)=0\) when \(p=q\). Note that it is not symmetric, so it can not directly be used as a metric.</p> <p>The idea of the natural gradient is to use the KL divergence as a regularizer when doing gradient descent. We will denote by \(p_{\theta}\) a parametric model and \(\Delta\theta\) a change in its parameter values. \(\text{KL}\left(p_{\theta}\parallel p_{\theta+\Delta\theta}\right)\) is used as our regularizer, so that each change \(\Delta\theta\) gives the same change in the distribution space. Instead of using the full expression for \(\text{KL}\left(p_{\theta}\parallel p_{\theta+\Delta\theta}\right)\) we will use its second order Taylor series around \(\theta\) (for full derivation see for instance <span class="bibcites">[<a class="bibliocite" name="cite-3" href="#biblio-3"><span class="bib-index">3</span></a>]</span>):</p> \[\begin{eqnarray*} \text{KL}\left(p_{\theta}\Vert p_{\theta+\Delta\theta}\right) &amp; = &amp; \Delta\theta^{T}\mathbf{F}\Delta\theta+o(\left\Vert \Delta\theta\right\Vert _{2}^{2}) \end{eqnarray*}\] <p>This expression exhibits the FIM which can now be used directly as a regularizer.</p> <h3 id="natural-gradient-descent">(Natural) gradient descent</h3> <p>The usual gradient descent algorithm can be formulated as the minimization of the following expression:</p> \[\begin{eqnarray*} \Delta\theta &amp; = &amp; \text{argmin}_{\Delta\theta}\left\{ \Delta\theta^{T}\nabla\mathcal{L}+\frac{\epsilon}{2}\left\Vert \Delta\theta\right\Vert ^{2}\right\} \end{eqnarray*}\] <p>We use the notation \(\mathcal{L}\) for the expectation of the loss function over the distribution of the data. This expression can be easily solved giving the expression \(\Delta\theta=-\frac{1}{\lambda}\nabla\mathcal{L}\). The parameter \(\epsilon\) is the inverse of the learning rate, and controls how much each parameter can change. We will now add a new regularizer using the FIM, and transform the minimization problem into:</p> \[\begin{eqnarray*} \Delta\theta &amp; = &amp; \text{argmin}_{\Delta\theta}\left\{ \Delta\theta^{T}\nabla\mathcal{L}+\frac{\epsilon}{2}\left\Vert \Delta\theta\right\Vert ^{2}+\frac{\lambda}{2}\Delta\theta^{T}\mathbf{F}\Delta\theta\right\} \end{eqnarray*}\] <p>We now constrain our gradient step to be small in term of change of parameter values, and also to be small in term of how much the resulting distribution changes. This expression can be solved to give \(\Delta\theta=\frac{1}{\lambda}\left(\mathbf{F}+\epsilon\mathbf{I}\right)^{-1}\nabla\mathcal{L}\). This expression also gives an insight for the role of \(\lambda\) and \(\epsilon\), which control 2 different but related quantities expressed by our constraints. This new update is called the natural gradient.</p> <h2 id="factorizing-the-fim-for-neural-networks">Factorizing the FIM for neural networks</h2> <h3 id="an-expression-for-the-fim-using-jacobians">An expression for the FIM using jacobians</h3> <p>Interestingly, for the usual distributions expressed by neural networks, the FIM takes the following simple form as shown by <span class="bibcites">[<a class="bibliocite" name="cite-3" href="#biblio-3"><span class="bib-index">3</span></a>]</span>:</p> \[\begin{eqnarray*} \mathbf{F} &amp; = &amp; \mathbb{E}_{x\sim q}\left[\boldsymbol{J}_{\boldsymbol{y}\left(x\right)}^{T}D\left(\boldsymbol{y}\left(x\right)\right)\boldsymbol{J}_{\boldsymbol{y}\left(x\right)}\right] \end{eqnarray*}\] <p>The values for \(x\) are drawn from the data generating distribution \(q\). The notation \(\boldsymbol{J}_{\boldsymbol{y}\left(x\right)}\) is used for the jacobian of the output of the network (i.e. the probability expressed at a given \(x\) : \(p\left(y\mid x\right)\)), with respect to the parameters. In other words, it measures how much the output of the network will change for a given \(x\) if we change the parameters. \(D\) is a diagonal matrix with non negative diagonal terms, and depends of the cost function used. For the quadratic loss it is the identity.</p> <h3 id="approximations-to-the-fim">Approximations to the FIM</h3> <p>The FIM is difficult to compute because of its size (\(n_{parameters}\times n_{parameters}\)) and because in general we do not have an expression for \(q\) but only samples from a training dataset.</p> <p>A first approximation that we can make is by ignoring the interactions between layers. In this case the FIM takes the form of a block diagonal matrix, where each block is a square matrix which has the size of the parameters of a layer. For a neural network with \(n_{layers}\) layers this reduces the FIM into \(n_{layers}\) smaller matrices. We will denote by \(\mathbf{F}_{i}\) the block corresponding to layer \(i\).</p> <p>A second common approximation we make in practice is to use the empirical FIM for a training dataset of \(n\) examples \(x_{i}\): \(\mathbf{F}=\frac{1}{n}\sum_{i}\boldsymbol{J}_{\boldsymbol{y}\left(x_{i}\right)}^{T}D\left(\boldsymbol{y}\left(x_{i}\right)\right)\boldsymbol{J}_{\boldsymbol{y}\left(x_{i}\right)}\).</p> <h3 id="factorization-of-the-mathbff_is-using-the-kronecker-product">Factorization of the \(\mathbf{F}_{i}\)s using the Kronecker product</h3> <p>In the rest of this note, we will restrict our analysis to a multilayer perceptron. Each layer is parametrized using a weight matrix \(W\) of size \(\left(\text{out}\times\text{in}\right)\) and a bias vector \(b\) of size \(\left(\text{out}\right)\). A layer consists in a linear transformation and a non-linearity \(f\) to give the hidden representation of the next layer:</p> \[\begin{eqnarray*} h_{l+1}=f_{l}\left(a_{l}\right) &amp; \text{with} &amp; a_{l}=W_{l}h_{l}+b_{l} \end{eqnarray*}\] <p>We will focus on a single layer, and drop the subscript \(l\). In the following, we will also focus on a single example in the expectation for the FIM. Each individual example has its own jacobian with respect to the parameter. In other words, this jacobian measures how the output of the network changes for this example, if you move the parameter values.</p> <p>The jacobians for parameters \(W\) and \(b\) are obtained using the chain rule for derivation: \(\boldsymbol{J}_{\boldsymbol{y}}^{W}=\boldsymbol{J}_{\boldsymbol{y}}^{a}\boldsymbol{J}_{a}^{W}\) and \(\boldsymbol{J}_{\boldsymbol{y}}^{b}=\boldsymbol{J}_{\boldsymbol{y}}^{a}\boldsymbol{J}_{a}^{b}\). The jacobian for the bias simplifies as \(\boldsymbol{J}_{a}^{b}=\mathbf{I}\). For the weight matrix, it is a little bit more tricky. As \(W\) is a matrix and not a vector, we can not express a jacobian matrix directly. We will have to make use of the Kronecker product and the \(\text{vec}\) operator. We start from the linear relation \(a=Wh+b\) and remark that \(a=\text{vec}\left(a\right)\) since \(a\) is a vector. We can now make use of the formula \(\text{vec}\left(AXB\right)=\left(B^{T}\otimes A\right)\text{vec}\left(X\right)\) to obtain:</p> \[\begin{eqnarray*} a &amp; = &amp; \left(h^{T}\otimes\mathbf{I}_{out}\right)\text{vec}\left(W\right)\\ \boldsymbol{J}_{a}^{\text{vec}\left(W\right)} &amp; = &amp; \left(h^{T}\otimes\mathbf{I}_{out}\right) \end{eqnarray*}\] <p>Putting everything together we get the jacobians:</p> \[\begin{eqnarray*} \boldsymbol{J}_{\boldsymbol{y}}^{\text{vec}\left(W\right)} &amp; = &amp; \boldsymbol{J}_{\boldsymbol{y}}^{a}\left(h^{T}\otimes\mathbf{I}_{out}\right)\\ &amp; = &amp; h^{T}\otimes\boldsymbol{J}_{\boldsymbol{y}}^{a}\\ \boldsymbol{J}_{\boldsymbol{y}}^{b} &amp; = &amp; \boldsymbol{J}_{\boldsymbol{y}}^{a} \end{eqnarray*}\] <p>Now imagine that we stack all parameters \(W\) and \(b\) in a vector \(\theta=\left(\text{vec}\left(W\right)_{1}\cdots\text{vec}\left(W\right)_{in\times out}b_{1}\cdots b_{out}\right)\). We get the full jacobian by stacking the jacobians:</p> \[\begin{eqnarray*} \boldsymbol{J}_{\boldsymbol{y}}^{\theta} &amp; = &amp; \left(\begin{array}{cc} h^{T} &amp; 1\end{array}\right)\otimes\boldsymbol{J}_{\boldsymbol{y}}^{a} \end{eqnarray*}\] <p>From this expression we can finally express the FIM in a factorized form:</p> \[\begin{eqnarray} \mathbf{F} &amp; = &amp; \mathbb{E}\left[\left\{ \left(\begin{array}{c} h\\ 1 \end{array}\right)\otimes\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}\right\} D\left(\boldsymbol{y}\right)\left\{ \left(\begin{array}{cc} h^{T} &amp; 1\end{array}\right)\otimes\boldsymbol{J}_{\boldsymbol{y}}^{a}\right\} \right]\nonumber \\ &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc} hh^{T} &amp; h\\ h^{T} &amp; 1 \end{array}\right)\otimes\left(\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}D\left(\boldsymbol{y}\right)\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)\right]\label{eq:factoredfim} \end{eqnarray}\] <p>In this expression, remember that all 3 variables \(h\), \(a\) and \(\boldsymbol{y}\) have a different value for each individual example. The FIM is the sum of the contributions of each example. The use of the Kronecker product permits splitting the contribution of each example in a term that involves the input of the layer \(\left(\begin{array}{cc} hh^{T} &amp; h\\ h^{T} &amp; 1 \end{array}\right)\) and a term that involves the jacobian received on the output of the linear transformation \(\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}D\left(\boldsymbol{y}\right)\boldsymbol{J}_{\boldsymbol{y}}^{a}\).</p> <h2 id="kfac">KFAC</h2> <p><span class="bibcites">[<a class="bibliocite" name="cite-2" href="#biblio-2"><span class="bib-index">2</span></a>]</span> use a simplification of the FIM that drastically reduce the computation required for inverting it. Remember that even if we consider a block diagonal approximation of the FIM, each block still has size \(n_{parameters}\times n_{parameters}\). We need to invert this block, and the operation of inverting a square matrix is \(O\left(n_{parameters}^{3}\right)\). They propose the following approximation:</p> \[\begin{eqnarray*} \mathbf{F} &amp; \approx &amp; \mathbb{E}\left[\left(\begin{array}{cc} hh^{T} &amp; h\\ h^{T} &amp; 1 \end{array}\right)\right]\otimes\mathbb{E}\left[\left(\boldsymbol{J}_{\boldsymbol{y}}^{a}\right)^{T}D\left(\boldsymbol{y}\right)\boldsymbol{J}_{\boldsymbol{y}}^{a}\right] \end{eqnarray*}\] <p>The Kronecker product has the nice property that for 2 invertible square matrices \(A\) and \(B\), \(\left(A\otimes B\right)^{-1}=A^{-1}\otimes B^{-1}\). It follows that inverting the FIM now requires inverting 2 smaller matrices. However the approximation they use is questionable, and they show some interesting experimental results.</p> <h2 id="natural-neural-networks">Natural Neural Networks</h2> <p><span class="bibcites">[<a class="bibliocite" name="cite-1" href="#biblio-1"><span class="bib-index">1</span></a>]</span> exploit the same factorization <a class="Reference" href="#eq:factoredfim">↓</a> but only consider the term involving the input of the linear transformation (\(h\)). They propose a reparametrization that will make \(\mathbb{E}\left[\left(\begin{array}{cc} hh^{T} &amp; h\\ h^{T} &amp; 1 \end{array}\right)\right]\) equal the identity. To this view, they change the original linear transformation \(a=Wh+b\) to become:</p> \[\begin{eqnarray*} a &amp; = &amp; VU\left(h-\mu\right)+d \end{eqnarray*}\] <p>\(V\) is the new weight matrix and \(d\) are the new biases. \(\mu=\mathbb{E}\left[h\right]\) is the mean value for \(h\) and \(U\) is the square root of the inverse covariance of \(h\), defined by \(U^{2}=\left(\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]\right)^{-1}\), denoted by \(U=\left(\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]\right)^{-\frac{1}{2}}\). \(U\) and \(\mu\) are not trained using gradient descent but instead they are estimated using data from the training set.</p> <p>Our new parameters \(V\) and \(d\) are trained using gradient descent, which will now have the desired property. We will denote by \(h_{e}=U\left(h-\mu\right)\) our new “effective” input to the linear transformation induced by the weight matrix \(V\). Let us first remark that \(\mathbb{E}\left[h_{e}\right]=U\left(\mathbb{E}\left[h\right]-\mu\right)=U\left(\mu-\mu\right)=0\), so the new input is centered on average. A second remark is that \(\mathbb{E}\left[h_{e}h_{e}^{T}\right]=U\mathbb{E}\left[\left(h-\mu\right)\left(h-\mu\right)^{T}\right]U^{T}=\mathbf{I}\). By construction \(U\) cancels out the covariance. Wrapping everything together we thus have the desired property that:</p> \[\begin{eqnarray*} \mathbb{E}\left[\left(\begin{array}{cc} h_{e}h_{e}^{T} &amp; h_{e}\\ h_{e}^{T} &amp; 1 \end{array}\right)\right] &amp; = &amp; \left(\begin{array}{cc} \mathbb{E}\left[h_{e}h_{e}^{T}\right] &amp; \mathbb{E}\left[h_{e}\right]\\ \mathbb{E}\left[h_{e}^{T}\right] &amp; 1 \end{array}\right)\\ &amp; = &amp; \left(\begin{array}{cc} \mathbf{I} &amp; 0\\ 0 &amp; 1 \end{array}\right) \end{eqnarray*}\] <p>The FIM for our new reparametrization thus has a better form, and they also show experimentally that this method is very efficient, and that by amortizing the cost of inverting the covariance matrix over several parameter updates, it can be faster that standard SGD.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog note we showed the path from the intuituion behind using the FIM as a preconditioner for gradient descent, to 2 recent algorithms that use the FIM. We also introduced the complete form for the FIM, also including the bias, which in the best of our knowledge has not been explicited in any publication at this time.</p> <p>We think that the FIM or other similar preconditioners, and their factorizations, will open a new serie of algorithms to optimize neural networks, that will help make progress in artificial intelligence tasks.</p> <h2 id="references">References</h2> <p class="biblio"> <span class="entry">[<a class="biblioentry" name="biblio-1"><span class="bib-index">1</span></a>] </span> <span class="bib-authors">Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, others</span>: “<span class="bib-title">Natural neural networks</span>”, <i><span class="bib-booktitle">Advances in Neural Information Processing Systems</span></i>, pp. <span class="bib-pages">2071—2079</span>, <span class="bib-year">2015</span>. </p> <p class="biblio"> <span class="entry">[<a class="biblioentry" name="biblio-2"><span class="bib-index">2</span></a>] </span> <span class="bib-authors">James Martens, Roger B Grosse</span>: “<span class="bib-title">Optimizing Neural Networks with Kronecker-factored Approximate Curvature.</span>”, <i><span class="bib-booktitle">ICML</span></i>, pp. <span class="bib-pages">2408—2417</span>, <span class="bib-year">2015</span>. </p> <p class="biblio"> <span class="entry">[<a class="biblioentry" name="biblio-3"><span class="bib-index">3</span></a>] </span> <span class="bib-authors">Razvan Pascanu, Yoshua Bengio</span>: “<span class="bib-title">Revisiting natural gradient for deep networks</span>”, <i><span class="bib-journal">arXiv preprint arXiv:1301.3584</span></i>, <span class="bib-year">2013</span>. </p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/batchnorm-derivatives/">Derivatives through a batch norm layer</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/empirical-fisher/">What is the empirical Fisher ?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/fisher-conditional/">How to compute the Fisher of a conditional when applying natural gradient to neural networks?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/algebra2ndorder/">The algebra of second order methods in neural networks</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Thomas George. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-99149577-1"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-99149577-1");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications and pre-prints, also see my google scholar profile",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-software",title:"software",description:"Pieces of code I developed during my research and that I am releasing open-source. For a complete list go to my github page.",section:"Navigation",handler:()=>{window.location.href="/software/"}},{id:"post-derivatives-through-a-batch-norm-layer",title:"Derivatives through a batch norm layer",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/batchnorm-derivatives/"}},{id:"post-what-is-the-empirical-fisher",title:"What is the empirical Fisher ?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/empirical-fisher/"}},{id:"post-how-to-compute-the-fisher-of-a-conditional-when-applying-natural-gradient-to-neural-networks",title:"How to compute the Fisher of a conditional when applying natural gradient to...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/fisher-conditional/"}},{id:"post-the-algebra-of-second-order-methods-in-neural-networks",title:"The algebra of second order methods in neural networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/algebra2ndorder/"}},{id:"post-demystifying-natural-neural-networks",title:"Demystifying Natural Neural Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/demystifying-natural-nn/"}},{id:"news-i-will-give-a-talk-titled-optimization-and-generalization-through-the-lens-of-the-linearization-of-neural-networks-training-dynamics-in-front-of-roger-grosse-https-www-cs-toronto-edu-rgrosse-39-s-group-at-vector-institute-toronto",title:"I will give a talk titled _Optimization and generalization through the lens of...",description:"",section:"News"},{id:"news-i-will-present-nngeometry-https-github-com-tfjgeorge-nngeometry-at-the-pytorch-ecosystem-day-https-pytorch-org-ecosystem-pted-2021",title:"I will present [NNGeometry](https://github.com/tfjgeorge/nngeometry/) at the [Pytorch Ecosystem Day](https://pytorch.org/ecosystem/pted/2021)",description:"",section:"News"},{id:"news-presentation-of-implicit-regularization-via-neural-feature-alignment-at-conf\xe9rence-sur-l-39-apprentissage-automatique-2021-https-cap2021-sciencesconf-org-resource-page-id-8-html",title:"Presentation of _Implicit Regularization via Neural Feature Alignment_ at [Conf\xe9rence sur l&#39;Apprentissage Automatique...",description:"",section:"News"},{id:"news-presentation-of-our-workshop-paper-continual-learning-and-deep-networks-an-analysis-of-the-last-layer-with-timoth\xe9e-lesort-at-the-theory-of-continual-learning-workshop-at-icml",title:"Presentation of our workshop paper *Continual learning and Deep Networks: an Analysis of...",description:"",section:"News"},{id:"news-i-will-present-our-recent-work-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-paper-assets-pdf-lazy-vs-hasty-icml-scis-2022-pdf-poster-assets-pdf-lazy-vs-hasty-icml-scis-2022-poster-pdf-at-the-scis-https-sites-google-com-view-scis-workshop-home-workshop-at-icml-2022",title:"I will present our recent work _Lazy vs hasty: linearization in deep networks...",description:"",section:"News"},{id:"news-new-pre-print-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-https-arxiv-org-abs-2209-09658-available",title:"New pre-print [Lazy vs hasty: linearization in deep networks impacts learning schedule based...",description:"",section:"News"},{id:"news-our-paper-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-https-openreview-net-pdf-id-lukvf4vrfp-was-accepted-to-tmlr-https-openreview-net-forum-id-lukvf4vrfp-code-https-github-com-tfjgeorge-lazy-vs-hasty",title:"Our paper [Lazy vs hasty: linearization in deep networks impacts learning schedule based...",description:"",section:"News"},{id:"news-i-successfully-defended-my-phd-thesis-deep-networks-training-and-generalization-insights-from-linearization-manuscript-assets-pdf-thomas-george-phd-thesis-pdf-slides-assets-pdf-phd-presentation-thomas-george-pdf",title:"I successfully defended my PhD Thesis: **Deep networks training and generalization: insights from...",description:"",section:"News"},{id:"news-i-am-now-a-postdoctoral-researcher-at-orange-labs-https-hellofuture-orange-com-fr-in-vincent-lemaire-http-vincentlemaire-labs-fr-39-s-group",title:"I am now a postdoctoral researcher at [Orange Labs](https://hellofuture.orange.com/fr/) in [Vincent Lemaire](http://vincentlemaire-labs.fr/)&#39;s group....",description:"",section:"News"},{id:"news-i-will-present-our-paper-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-https-openreview-net-pdf-id-lukvf4vrfp-at-conf\xe9rence-sur-l-39-apprentissage-automatique-cap-https-pfia23-icube-unistra-fr-conferences-cap-index-html-in-strasbourg",title:"I will present our paper [Lazy vs hasty: linearization in deep networks impacts...",description:"",section:"News"},{id:"news-i-joined-orange-research-https-hellofuture-orange-com-fr-as-a-permanent-research-scientist-in-the-responsible-and-impactful-ai-program",title:"I joined [Orange Research](https://hellofuture.orange.com/fr/) as a permanent research scientist in the Responsible and...",description:"",section:"News"},{id:"news-our-paper-mislabeled-examples-detection-viewed-as-probing-machine-learning-models-concepts-survey-and-extensive-benchmark-https-openreview-net-pdf-id-3ylor7bhkx-was-accepted-to-tmlr-https-openreview-net-forum-id-3ylor7bhkx-code-https-github-com-orange-opensource-mislabeled-video-https-youtu-be-ft9vzxs0nh8",title:"Our paper [Mislabeled examples detection viewed as probing machine learning models: concepts, survey...",description:"",section:"News"},{id:"news-together-with-pierre-nodet-https-scholar-google-com-citations-user-ido8qgeaaaaj-we-will-present-our-recent-works-on-weakly-supervised-learning-at-the-lfi-lip6-seminary-https-lfi-lip6-fr-seminaires-apprentissage-faiblement-supervis\xe9-algorithmes-biqualit\xe9-et-d\xe9tection-automatis\xe9e-d-39-exemples-mal-\xe9tiquet\xe9s",title:"Together with [Pierre Nodet](https://scholar.google.com/citations?user=ido8qGEAAAAJ), we will present our recent works on Weakly supervised...",description:"",section:"News"},{id:"news-i-will-attend-egc-https-www-egc2025-cnrs-fr-in-strasbourg-to-present-our-paper-calibration-des-mod\xe8les-d-apprentissage-pour-l-am\xe9lioration-des-d\xe9tecteurs-automatiques-d-exemples-mal-\xe9tiquet\xe9s-assets-pdf-mislabeled-calibration-paper-fr-pdf-as-well-as-the-workshop-iacd-https-sites-google-com-view-atelier-iacd-2025-for-a-presentation-of-our-paper-mislabeled-examples-detection-viewed-as-probing-machine-learning-models-concepts-survey-and-extensive-benchmark-https-openreview-net-pdf-id-3ylor7bhkx",title:"I will attend [EGC](https://www.egc2025.cnrs.fr/) in Strasbourg to present our paper [Calibration des mod\xe8les...",description:"",section:"News"},{id:"news-new-post-on-orange-research-39-s-hello-future-blog-on-how-to-make-ai-systems-explainable-fran\xe7ais-https-hellofuture-orange-com-fr-comment-rendre-lia-explicable-english-https-hellofuture-orange-com-en-how-to-make-ai-explainable",title:"New post on Orange Research&#39;s Hello Future blog on how to make AI...",description:"",section:"News"},{id:"news-phd-position-at-orange-research-with-pierre-nodet-https-scholar-google-com-citations-user-ido8qgeaaaaj-and-me-explaining-black-box-ai-algorithms-through-their-training-examples-en-https-orange-jobs-jobs-v3-offers-145586-lang-en-fr-https-orange-jobs-jobs-v3-offers-145586-lang-fr",title:"PhD position at Orange Research with [Pierre Nodet](https://scholar.google.com/citations?user=ido8qGEAAAAJ) and me: *Explaining \u201cblack box\u201d...",description:"",section:"News"},{id:"news-i-will-attend-the-trustworthy-ai-summit-https-www-trustworthy-ai-summit-eu-with-jeanne-monnier-to-present-our-work-fairness-in-intersectional-setups-aggregation-choice-and-some-paradoxes",title:"I will attend the [Trustworthy AI Summit](https://www.trustworthy-ai-summit.eu/) with Jeanne Monnier to present our...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%74%66%6A%67%65%6F%72%67%65@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=pc3_ujYAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/tfjgeorge","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/tfjgeorge","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/https://bsky.app/profile/tfjgeorge.bsky.social","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>