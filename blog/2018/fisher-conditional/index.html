<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>How to compute the Fisher of a conditional when applying natural gradient to neural networks? | Thomas  George</title>
    <meta name="author" content="Thomas  George">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tfjgeorge.github.io/blog/2018/fisher-conditional/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Thomas </span>George</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/software/">software</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">How to compute the Fisher of a conditional when applying natural gradient to neural networks?</h1>
    <p class="post-meta">October 29, 2018</p>
    <p class="post-tags">
      <a href="/blog/2018"> <i class="fas fa-calendar fa-sm"></i> 2018 </a>
        ·  
        <a href="/blog/category/note">
          <i class="fas fa-tag fa-sm"></i> note</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>This short note aims at explaining how we come up with an expression for the Fisher Information Matrix in the context of the conditional distributions represented by neural networks.</p>

<p>In neural networks, the so called natural gradient is a preconditioner for the gradient descent algorithm, where the update is regularized so that each update \(\Delta\theta\) of the values of the parameters \(\theta\) will be measured using the \(KL\) divergence. This has some interesting properties, such as the effect of making the update invariant to reparametrization of our neural network: more explanation to come in another blog post. The update is given by:</p>

\[\begin{eqnarray*}
\Delta_{nat}\theta &amp; = &amp; -\mathbf{F}_{\theta}^{-1}\mathbb{E}_{\left(x,y\right)\sim\mathcal{D}_{train}}\left[\nabla_{\theta}\left\{ -\log p_{\theta}\left(y|x\right)\right\} \right]
\end{eqnarray*}\]

<p>where:</p>

<ul>
<li>
the expectation is taken using (discrete) samples \(\left(x,y\right)\) of the training set \(\mathcal{D}_{train}\);
</li>
<li>
\(p_{\theta}\left(y|x\right)\) is our neural network with \(x\) the input (e.g. the pixels of an image), and \(y\) the output (e.g. the 10 coefficients of the softmax for MNIST where we have 10 classes = 10 digits);
</li>
<li>
we use the negative log likelihood as our loss function \(-\log p_{\theta}\left(y|x\right)\), and so \(\nabla_{\theta}\left\{ -\log p_{\theta}\left(y|x\right)\right\} \) is the gradient of our loss with respect to the parameters \(\theta\);
</li>
<li>
\(\mathbf{F}_{\theta}\) is the Fisher Information Matrix (FIM) , defined as:
</li>

</ul>

\[\begin{eqnarray*}
\mathbf{F}_{\theta} &amp; = &amp; \mathbb{E}_{z\sim p_{\theta}\left(z\right)}\left[\frac{\partial\log p_{\theta}\left(z\right)}{\partial\theta}\left(\frac{\partial\log p_{\theta}\left(z\right)}{\partial\theta}\right)^{\top}\right]
\end{eqnarray*}\]

<p>The link between the \(KL\) and the FIM resides in the fact that the FIM is the second order term of the Taylor series expansion of the \(KL\): For a distribution \(p_{\theta}\left(z\right)\) it is given by:</p>

\[\begin{eqnarray*}
KL\left(p_{\theta}\left(z\right)\parallel p_{\theta+\Delta\theta}\left(z\right)\right) &amp; = &amp; \Delta\theta^{\top}\mathbf{F}_{\theta}\Delta\theta+o\left(\left\Vert \Delta\theta\right\Vert _{2}^{2}\right)
\end{eqnarray*}\]

<p>where \(o\left(\left\Vert \Delta\theta\right\Vert _{2}^{2}\right)\) is negligible compared to \(\Delta\theta^{\top}\mathbf{F}\Delta\theta\) when \(\left\Vert \Delta\theta\right\Vert _{2}\) is small, the first order term is \(0\).</p>

<p>This is the general definition for \(\mathbf{F}_{\theta}\), using a density \(p_{\theta}\left(z\right)\). But when applying this technique to train neural networks, we model the conditional \(p_{\theta}\left(y\vert x\right)\). So how do we apply this to neural networks training, i.e. for the conditional \(p_{\theta}\left(y\vert x\right)\)?</p>

<p>Here is my explanation.</p>

<p>Instead of just considering \(p_{\theta}\left(y\vert x\right)\) we will use the joint probability \(p_{\theta}\left(y,x\right)=p_{\theta}\left(y\vert x\right)p\left(x\right)\). We have introduced \(p\left(x\right)\) which is the distribution over the inputs. If the task is image classification, this is the distribution of the natural images \(x\). Usually we do not have access to \(p\left(x\right)\) explicitely, but instead we have samples from it, which are the images in our training set.</p>

<p>By replacing \(p_{\theta}\left(z\right)\) with \(p_{\theta}\left(x,y\right)\) in the formula above, we can consider \(KL\left(p_{\theta}\left(x,y\right)\parallel p_{\theta+\Delta\theta}\left(x,y\right)\right)\) and write the FIM for this joint distribution:</p>

\[\begin{eqnarray*}
\mathbf{F}_{\theta} &amp; = &amp; \mathbb{E}_{\left(x,y\right)\sim p_{\theta}\left(x,y\right)}\left[\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\left(\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\right)^{\top}\right]
\end{eqnarray*}\]

<p>Next we replace the joint with the product of the marginal over \(x\) and the conditional in the derivative:</p>

\[\begin{eqnarray*}
\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta} &amp; = &amp; \frac{\partial\log\left(p_{\theta}\left(y|x\right)p\left(x\right)\right)}{\partial\theta}\\
 &amp; = &amp; \frac{\partial\left(\log p_{\theta}\left(y|x\right)+\log p\left(x\right)\right)}{\partial\theta}\\
 &amp; = &amp; \frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}+\frac{\partial\log p\left(x\right)}{\partial\theta}
\end{eqnarray*}\]

<p>and since \(p\left(x\right)\) does not depend on \(\theta\) then \(\frac{\partial\log p\left(x\right)}{\partial\theta}=0\). This simplifies in:</p>

\[\begin{eqnarray*}
\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta} &amp; = &amp; \frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}
\end{eqnarray*}\]

<p>Equivalently for the expectation, we can take the expectation in 2 steps:</p>

<ol>
<li>
sample a \(x\) from our training distribution;
</li>
<li>
for this value of \(x\) compute \(p_{\theta}\left(y|x\right)\) then sample multiple points to estimate the expectation over \(p_{\theta}\left(y|x\right)\). Here we also require multiple backprops to compute the gradients for each sample \(y\).
</li>

</ol>

<p>Finally we get the desired formula:</p>

\[\begin{eqnarray*}
\mathbf{F}_{\theta} &amp; = &amp; \mathbb{E}_{x\sim p\left(x\right),y\sim p_{\theta}\left(y|x\right)}\left[\frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}\left(\frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}\right)^{\top}\right]\\
 &amp; = &amp; \mathbb{E}_{x\sim p\left(x\right)}\left[\mathbb{E}_{y\sim p_{\theta}\left(y|x\right)}\left[\frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}\left(\frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}\right)^{\top}\right]\right]
\end{eqnarray*}\]

<p>And so we get the FIM for a conditional distribution.</p>


    </div>
  </article>


  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/batchnorm-derivatives/">Derivatives through a batch norm layer</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/empirical-fisher/">What is the empirical Fisher ?</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/algebra2ndorder/">The algebra of second order methods in neural networks</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/demystifying-natural-nn/">Demystifying Natural Neural Networks</a>
  </li>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Thomas  George. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-99149577-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-99149577-1');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
