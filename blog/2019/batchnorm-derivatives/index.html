<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Derivatives through a batch norm layer | Thomas George</title> <meta name="author" content="Thomas George"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://tfjgeorge.github.io/blog/2019/batchnorm-derivatives/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://tfjgeorge.github.io/"><span class="font-weight-bold">Thomas</span> George</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/software/">software</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Derivatives through a batch norm layer</h1> <p class="post-meta">February 15, 2019</p> <p class="post-tags"> <a href="/blog/2019"> <i class="fas fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/category/note"> <i class="fas fa-tag fa-sm"></i> note</a>   </p> </header> <article class="post-content"> <p>In this note, we will write the derivations for the backpropagated gradient through the batch norm operation, and also the gradient w.r.t the weight matrix. This derivation is very cumbersome and after having tried many different ways (column by column/line by line/element by element/etc), we present the way we think is the simplest one here.</p> <h2 id="notations">Notations</h2> <p>We focus on a single batch normalized layer in a fully connected network. The linear part is denoted \(y=Wx\) and parametrized by the weight matrix \(W\). Then the batch normalization operation is computed by:</p> \[\begin{align*} \hat{y}=BN\left(y\right) &amp; =\frac{y-\mu_{y}}{\sqrt{\text{var}\left(y\right)+\epsilon}} \end{align*}\] <p>Note that we can rewrite it in terms of \(x\) and \(W\) directly instead of computing the intermediate step \(y\):</p> \[\begin{align} \hat{y} &amp; =\frac{Wx-W\mu_{x}}{\sqrt{\text{var}\left(Wx\right)+\epsilon}}\nonumber \\ &amp; =\frac{W\left(x-\mu_{x}\right)}{\sqrt{\text{diag}\left(W^{\top}\text{cov}\left(x\right)W\right)+\epsilon}}\label{eq:bn_wx} \end{align}\] <p>Some remarks:</p> <ol> <li>These notations are not very precise since we mix up elementwise operations with linear algebra operations. Specifically, by an abuse of notation we divide a vector on the top part of the quotient, by another vector on the bottom part. </li> <li>Even if we only require to compute an elementwise variance of the components of \(y\), in <a class="Reference" href="#eq:bn_wx">\(\ref{eq:bn_wx}\)</a> we see that it hides the full covariance matrix on the vectors \(x\) in minibatches, here denoted by \(\text{cov}\). It is a dense covariance matrix, with size \(\text{in}\times\text{in}\).</li> <li>We did not write the scaling and bias parameters \(\gamma\) and \(\beta\) since obtaining their derivative is easier and less interesting.</li> </ol> <h2 id="minibatch-vector-notation">Minibatch vector notation</h2> <p>To clarify things, we consider that the examples are stacked in design matrices of size \(\text{batch size}\times\text{vector size}\):</p> <p>\(X=\left(\begin{array}{c} -\,x^{\left(1\right)\top}\,-\\ \vdots\\ -\,x^{\left(n\right)\top}\,- \end{array}\right)\), \(Y=\left(\begin{array}{c} -\,y^{\left(1\right)\top}\,-\\ \vdots\\ -\,y^{\left(n\right)\top}\,- \end{array}\right)\) and \(\hat{Y}=\left(\begin{array}{c} -\,\hat{y}^{\left(1\right)\top}\,-\\ \vdots\\ -\,\hat{y}^{\left(n\right)\top}\,- \end{array}\right)\)</p> <p>Using this notation, we can write the result of BN for a column of the matrix (so all \(i\)s component for all examples in a minibatch). We denote this column by \(y_{i}=Y_{:i}\), as opposed to the lines of \(Y\) that we denoted by \(y^{\left(j\right)\top}=Y_{j:}.\) Note that \(y_{i}\) does not correspond to an example in the minibatch.</p> <p>We will go step by step:</p> <p>The mean of a column is obtained by multiplying it with a vector full of \(1\)(denoted by a bold \(\boldsymbol{1}\)), and dividing by \(n\):</p> \[\begin{eqnarray*} \frac{1}{n}\sum_{t}\left(y_{i}\right)_{t} &amp; = &amp; \frac{1}{n}\boldsymbol{1}^{\top}y_{i} \end{eqnarray*}\] <p>Using this we can write the (unbiased) variance of the column vector \(y_{i}\):</p> \[\begin{eqnarray*} \text{var}\left(y_{i}\right) &amp; = &amp; \frac{1}{n-1}\left(y_{i}-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}y_{i}\right)^{\top}\left(y_{i}-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}y_{i}\right) \end{eqnarray*}\] <p>We multiplied the mean by a \(\boldsymbol{1}\) vector in order to repeat it along all components of \(y_{i}\). We can simplify the expression:</p> \[\begin{eqnarray*} \text{var}\left(y_{i}\right) &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\\ &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{2}{n}\boldsymbol{1}\boldsymbol{1}^{\top}+\frac{1}{n^{2}}\boldsymbol{1}\boldsymbol{1}^{\top}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\\ &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{2}{n}\boldsymbol{1}\boldsymbol{1}^{\top}+\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\\ &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i} \end{eqnarray*}\] <p>And so we obtain one column of batch norm:</p> \[\begin{eqnarray} \hat{y}_{i} &amp; = &amp; \frac{1}{\sqrt{\frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\label{eq:bn1} \end{eqnarray}\] <p>Using this notation we gained the fact that everything here is linear algebra and scalar operations. We do not have any more elementwise operations, nor sums or variance, so it is easier to write derivatives using only elementary calculus rules.</p> <h2 id="jacobians-and-gradients">Jacobians and gradients</h2> <p>Writing derivatives of vector functions with respect to vector parameters can be cumbersome, and sometimes ill-defined.</p> <p>In this note we follow the convention that a gradient of a scalar function of any object has the same shape as this object, so for instance \(\nabla_{W}\) as the same shape as \(W\).</p> <p>We also make an heavy use of jacobians, which are the matrices of partial derivatives. For a function \(f:\mathbb{R}^{m}\rightarrow\mathbb{R}^{n}\), its jacobian is a \(m\times n\) matrix, defined by:</p> \[\begin{eqnarray*} \left(\frac{\partial f\left(x\right)}{\partial x}\right)_{ij} &amp; = &amp; \frac{\partial f\left(x\right)_{i}}{\partial x_{j}} \end{eqnarray*}\] <p>Using this notation the chain rule can be written for a composition of function \(f=g\circ h\):</p> \[\frac{\partial f\left(x\right)}{\partial x}=\frac{\partial g\left(h\left(x\right)\right)}{\partial x}=\frac{\partial g\left(h\right)}{\partial h}\frac{\partial h\left(x\right)}{\partial x}\] <p>Using this notation it is also easy to write first order Taylor series expansion of vector functions. The first order term is just the jacobian matrix, that we can multiply to the right by an increment \(dx\):</p> \[f\left(x+dx\right)=f\left(x\right)+\frac{\partial f\left(x\right)}{\partial x}dx+o\left(dx\right)\] <p>Since \(\frac{\partial f\left(x\right)}{\partial x}\) is a \(m\times n\) matrix then \(\frac{\partial f\left(x\right)}{\partial x}dx\) is a \(m\times1\) column vector so it lives in the same space as \(f\). Everything works out fine!</p> <h2 id="derivative-wrt-y">Derivative w.r.t \(y\)</h2> <p>We start by computing the derivative through the BN operation. One of the weakness of BN is that each batch normalized feature will be a function of all other elements in a minibatch, because of the mean and variance. This is why we will focus on a single column of the design matrix \(\hat{Y}\): in this case all elements of this column only depend on the elements of the corresponding column in \(\hat{Y}\).</p> <p>We write the derivative using the expression in <a class="Reference" href="#eq:bn1">\ref{eq:bn1}</a>.</p> \[\begin{eqnarray*} \frac{\partial\hat{y}_{i}}{\partial y_{i}} &amp; = &amp; \frac{1}{\sqrt{\frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{2}\frac{1}{\left(\frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}+\epsilon\right)^{\frac{3}{2}}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\frac{2}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)\\ &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right) \end{eqnarray*}\] <p>By</p> \[\begin{eqnarray*} \frac{\partial L}{\partial y_{i}} &amp; = &amp; \frac{\partial L}{\partial\hat{y}_{i}}\frac{\partial\hat{y}_{i}}{\partial y_{i}}\\ &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\frac{\partial L}{\partial\hat{y}_{i}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right)\\ &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\left(\frac{\partial L}{\partial\hat{y}_{i}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{i}\hat{y}_{i}^{\top}\right) \end{eqnarray*}\] <p>Note that \(\frac{\partial L}{\partial y_{i}}\) and \(\frac{\partial L}{\partial\hat{y}_{i}}\) are row vectors.</p> <h2 id="derivative-wrt-y-1">Derivative w.r.t \(Y\)</h2> <p>For efficient implementation, it is often more efficient to work with design matrices of size \(n\times d\) where \(n\) is the size of the minibatch, and \(d\) is the feature size. With some algebraic manipulation we write the gradient for all elements in the design matrix:</p> \[\begin{eqnarray*} \nabla_{Y} &amp; = &amp; \left(\begin{array}{ccc} | &amp; &amp; |\\ \left(\frac{\partial L}{\partial y_{1}}\right)^{\top} &amp; &amp; \left(\frac{\partial L}{\partial y_{n}}\right)^{\top}\\ | &amp; &amp; | \end{array}\right)\\ &amp; = &amp; \left(\begin{array}{c} -\,\frac{\partial L}{\partial y_{1}}\,-\\ \vdots\\ -\,\frac{\partial L}{\partial y_{n}}\,- \end{array}\right)^{\top}\\ &amp; = &amp; \left(\begin{array}{c} \frac{1}{\sqrt{\text{var}\left(y_{1}\right)+\epsilon}}\left(\frac{\partial L}{\partial\hat{y}_{1}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\frac{\partial L}{\partial\hat{y}_{1}}\hat{y}_{1}\hat{y}_{1}^{\top}\right)\\ \vdots\\ \frac{1}{\sqrt{\text{var}\left(y_{n}\right)+\epsilon}}\left(\frac{\partial L}{\partial\hat{y}_{n}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\frac{\partial L}{\partial\hat{y}_{n}}\hat{y}_{n}\hat{y}_{n}^{\top}\right) \end{array}\right)^{\top}\\ &amp; = &amp; \left(\left(\begin{array}{c} -\,\frac{\partial L}{\partial\hat{y}_{i}}\,-\\ \vdots\\ -\,\frac{\partial L}{\partial\hat{y}_{i}}\,- \end{array}\right)\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\left(\begin{array}{ccc} \frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{1} &amp; 0 &amp; 0\\ 0 &amp; \ddots &amp; 0\\ 0 &amp; 0 &amp; \frac{\partial L}{\partial\hat{y}_{m}}\hat{y}_{m} \end{array}\right)\left(\begin{array}{c} -\,\hat{y}_{1}^{\top}\,-\\ \vdots\\ -\,\hat{y}_{m}^{\top}\,- \end{array}\right)\right)^{\top}\Sigma_{y}^{-1}\\ &amp; = &amp; \left(\nabla_{\hat{Y}}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-C\hat{Y}^{\top}\right)^{\top}\Sigma_{y}^{-1}\\ &amp; = &amp; \left(\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)\nabla_{\hat{Y}}-\hat{Y}C\right)^{\top}\Sigma_{y}^{-1} \end{eqnarray*}\] <p>we denoted by \(\Sigma_{y}^{-1}=\left(\begin{array}{ccc} \frac{1}{\sqrt{\text{var}\left(y_{1}\right)+\epsilon}}\\ &amp; \ddots\\ &amp; &amp; \frac{1}{\sqrt{\text{var}\left(y_{m}\right)+\epsilon}} \end{array}\right)\) the diagonal matrix of the inverse standard deviation as usually used in BN, and \(C=\frac{1}{n-1}\left(\begin{array}{ccc} \frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{1} &amp; 0 &amp; 0\\ 0 &amp; \ddots &amp; 0\\ 0 &amp; 0 &amp; \frac{\partial L}{\partial\hat{y}_{m}}\hat{y}_{m} \end{array}\right)\) is a diagonal matrix where the coefficients are the (scalar) covariances of the elements of \(\frac{\partial L}{\partial\hat{y}_{i}}\) and \(\hat{y}_{i}\).</p> <h2 id="derivative-wrt-one-line-of-the-weight-matrix">Derivative w.r.t one line of the weight matrix</h2> <p>Using the fact that \(Y=XW^{\top}\), we write \(y_{i}=\left(XW^{\top}\right)_{:i}=Xw_{i}\), where \(w_{i}^{\top}=W_{i:}\) is a line of the weight matrix (that we transpose to obtain a column vector). We can now write the derivative using the chain rule:</p> \[\begin{eqnarray*} \frac{\partial\hat{y}_{i}}{\partial w_{i}} &amp; = &amp; \frac{\partial\hat{y}_{i}}{\partial y_{i}}\frac{\partial y_{i}}{\partial w_{i}}\\ &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right)X \end{eqnarray*}\] \[\begin{eqnarray*} \frac{\partial L}{\partial w_{i}} &amp; = &amp; \frac{\partial L}{\partial\hat{y}_{i}}\frac{\partial\hat{y}_{i}}{\partial w_{i}}\\ &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\frac{\partial L}{\partial\hat{y}_{i}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right)X \end{eqnarray*}\] <h2 id="derivative-wrt-the-whole-matrix">Derivative w.r.t the whole matrix</h2> <p>Now we can stack all lines of the matrix in order to get the derivative for the whole weight matrix:</p> \[\begin{eqnarray*} \nabla_{W} &amp; = &amp; \Sigma_{y}^{-1}\left(\begin{array}{c} \frac{\partial L}{\partial\hat{y}_{1}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{1}\hat{y}_{1}^{\top}\right)X\\ \vdots\\ \frac{\partial L}{\partial\hat{y}_{m}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{m}\hat{y}_{m}^{\top}\right)X \end{array}\right)\\ &amp; = &amp; \Sigma_{y}^{-1}\left(\left(\begin{array}{c} \frac{\partial L}{\partial\hat{y}_{i}}\\ \vdots\\ \frac{\partial L}{\partial\hat{y}_{i}} \end{array}\right)\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)X-\frac{1}{n-1}\left(\begin{array}{ccc} \frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{1} &amp; 0 &amp; 0\\ 0 &amp; \ddots &amp; 0\\ 0 &amp; 0 &amp; \frac{\partial L}{\partial\hat{y}_{m}}\hat{y}_{m} \end{array}\right)\left(\begin{array}{c} \hat{y}_{1}^{\top}\\ \vdots\\ \hat{y}_{m}^{\top} \end{array}\right)X\right)\\ &amp; = &amp; \Sigma_{y}^{-1}\left(\nabla_{\hat{Y}}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-C\hat{Y}^{\top}\right)X \end{eqnarray*}\] <h2 id="derivative-wrt-the-input-of-the-batch-normalized-layer">Derivative w.r.t the input of the batch normalized layer</h2> <p>Using <a class="Reference" href="#eq:bn1">\ref{eq:bn1}</a> and \(Y=XW^{\top}\) we can write \(\nabla_{X}\) using the chain rule:</p> \[\begin{eqnarray*} \nabla_{X} &amp; = &amp; \Sigma_{y}^{-1}\left(\nabla_{\hat{Y}}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-C\hat{Y}^{\top}\right)W^{\top} \end{eqnarray*}\] <h2 id="wrap-up-and-acknowledgements">Wrap-up and acknowledgements</h2> <p>Now you have everything you need !</p> <p>Special thanks to César Laurent for the help and proofreading.</p> </article><div id="disqus_thread"></div> <script type="text/javascript">var disqus_shortname="tfjgeorge",disqus_identifier="/blog/2019/batchnorm-derivatives",disqus_title="Derivatives through a batch norm layer";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Thomas George. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-99149577-1"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-99149577-1");</script> </body> </html>