<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Derivatives through a batch norm layer | Thomas George </title> <meta name="author" content="Thomas George"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tfjgeorge.github.io/blog/2019/batchnorm-derivatives/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Thomas</span> George </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/software/">software </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Derivatives through a batch norm layer</h1> <p class="post-meta"> Created in February 15, 2019 </p> <p class="post-tags"> <a href="/blog/2019"> <i class="fa-solid fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/category/note"> <i class="fa-solid fa-tag fa-sm"></i> note</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In this note, we will write the derivations for the backpropagated gradient through the batch norm operation, and also the gradient w.r.t the weight matrix. This derivation is very cumbersome and after having tried many different ways (column by column/line by line/element by element/etc), we present the way we think is the simplest one here.</p> <h2 id="notations">Notations</h2> <p>We focus on a single batch normalized layer in a fully connected network. The linear part is denoted \(y=Wx\) and parametrized by the weight matrix \(W\). Then the batch normalization operation is computed by:</p> \[\begin{align*} \hat{y}=BN\left(y\right) &amp; =\frac{y-\mu_{y}}{\sqrt{\text{var}\left(y\right)+\epsilon}} \end{align*}\] <p>Note that we can rewrite it in terms of \(x\) and \(W\) directly instead of computing the intermediate step \(y\):</p> \[\begin{align} \hat{y} &amp; =\frac{Wx-W\mu_{x}}{\sqrt{\text{var}\left(Wx\right)+\epsilon}}\nonumber \\ &amp; =\frac{W\left(x-\mu_{x}\right)}{\sqrt{\text{diag}\left(W^{\top}\text{cov}\left(x\right)W\right)+\epsilon}}\label{eq:bn_wx} \end{align}\] <p>Some remarks:</p> <ol> <li>These notations are not very precise since we mix up elementwise operations with linear algebra operations. Specifically, by an abuse of notation we divide a vector on the top part of the quotient, by another vector on the bottom part. </li> <li>Even if we only require to compute an elementwise variance of the components of \(y\), in <a class="Reference" href="#eq:bn_wx">\(\ref{eq:bn_wx}\)</a> we see that it hides the full covariance matrix on the vectors \(x\) in minibatches, here denoted by \(\text{cov}\). It is a dense covariance matrix, with size \(\text{in}\times\text{in}\).</li> <li>We did not write the scaling and bias parameters \(\gamma\) and \(\beta\) since obtaining their derivative is easier and less interesting.</li> </ol> <h2 id="minibatch-vector-notation">Minibatch vector notation</h2> <p>To clarify things, we consider that the examples are stacked in design matrices of size \(\text{batch size}\times\text{vector size}\):</p> <p>\(X=\left(\begin{array}{c} -\,x^{\left(1\right)\top}\,-\\ \vdots\\ -\,x^{\left(n\right)\top}\,- \end{array}\right)\), \(Y=\left(\begin{array}{c} -\,y^{\left(1\right)\top}\,-\\ \vdots\\ -\,y^{\left(n\right)\top}\,- \end{array}\right)\) and \(\hat{Y}=\left(\begin{array}{c} -\,\hat{y}^{\left(1\right)\top}\,-\\ \vdots\\ -\,\hat{y}^{\left(n\right)\top}\,- \end{array}\right)\)</p> <p>Using this notation, we can write the result of BN for a column of the matrix (so all \(i\)s component for all examples in a minibatch). We denote this column by \(y_{i}=Y_{:i}\), as opposed to the lines of \(Y\) that we denoted by \(y^{\left(j\right)\top}=Y_{j:}.\) Note that \(y_{i}\) does not correspond to an example in the minibatch.</p> <p>We will go step by step:</p> <p>The mean of a column is obtained by multiplying it with a vector full of \(1\)(denoted by a bold \(\boldsymbol{1}\)), and dividing by \(n\):</p> \[\begin{eqnarray*} \frac{1}{n}\sum_{t}\left(y_{i}\right)_{t} &amp; = &amp; \frac{1}{n}\boldsymbol{1}^{\top}y_{i} \end{eqnarray*}\] <p>Using this we can write the (unbiased) variance of the column vector \(y_{i}\):</p> \[\begin{eqnarray*} \text{var}\left(y_{i}\right) &amp; = &amp; \frac{1}{n-1}\left(y_{i}-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}y_{i}\right)^{\top}\left(y_{i}-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}y_{i}\right) \end{eqnarray*}\] <p>We multiplied the mean by a \(\boldsymbol{1}\) vector in order to repeat it along all components of \(y_{i}\). We can simplify the expression:</p> \[\begin{eqnarray*} \text{var}\left(y_{i}\right) &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\\ &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{2}{n}\boldsymbol{1}\boldsymbol{1}^{\top}+\frac{1}{n^{2}}\boldsymbol{1}\boldsymbol{1}^{\top}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\\ &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{2}{n}\boldsymbol{1}\boldsymbol{1}^{\top}+\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\\ &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i} \end{eqnarray*}\] <p>And so we obtain one column of batch norm:</p> \[\begin{eqnarray} \hat{y}_{i} &amp; = &amp; \frac{1}{\sqrt{\frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\label{eq:bn1} \end{eqnarray}\] <p>Using this notation we gained the fact that everything here is linear algebra and scalar operations. We do not have any more elementwise operations, nor sums or variance, so it is easier to write derivatives using only elementary calculus rules.</p> <h2 id="jacobians-and-gradients">Jacobians and gradients</h2> <p>Writing derivatives of vector functions with respect to vector parameters can be cumbersome, and sometimes ill-defined.</p> <p>In this note we follow the convention that a gradient of a scalar function of any object has the same shape as this object, so for instance \(\nabla_{W}\) as the same shape as \(W\).</p> <p>We also make an heavy use of jacobians, which are the matrices of partial derivatives. For a function \(f:\mathbb{R}^{m}\rightarrow\mathbb{R}^{n}\), its jacobian is a \(m\times n\) matrix, defined by:</p> \[\begin{eqnarray*} \left(\frac{\partial f\left(x\right)}{\partial x}\right)_{ij} &amp; = &amp; \frac{\partial f\left(x\right)_{i}}{\partial x_{j}} \end{eqnarray*}\] <p>Using this notation the chain rule can be written for a composition of function \(f=g\circ h\):</p> \[\frac{\partial f\left(x\right)}{\partial x}=\frac{\partial g\left(h\left(x\right)\right)}{\partial x}=\frac{\partial g\left(h\right)}{\partial h}\frac{\partial h\left(x\right)}{\partial x}\] <p>Using this notation it is also easy to write first order Taylor series expansion of vector functions. The first order term is just the jacobian matrix, that we can multiply to the right by an increment \(dx\):</p> \[f\left(x+dx\right)=f\left(x\right)+\frac{\partial f\left(x\right)}{\partial x}dx+o\left(dx\right)\] <p>Since \(\frac{\partial f\left(x\right)}{\partial x}\) is a \(m\times n\) matrix then \(\frac{\partial f\left(x\right)}{\partial x}dx\) is a \(m\times1\) column vector so it lives in the same space as \(f\). Everything works out fine!</p> <h2 id="derivative-wrt-y">Derivative w.r.t \(y\)</h2> <p>We start by computing the derivative through the BN operation. One of the weakness of BN is that each batch normalized feature will be a function of all other elements in a minibatch, because of the mean and variance. This is why we will focus on a single column of the design matrix \(\hat{Y}\): in this case all elements of this column only depend on the elements of the corresponding column in \(\hat{Y}\).</p> <p>We write the derivative using the expression in <a class="Reference" href="#eq:bn1">\ref{eq:bn1}</a>.</p> \[\begin{eqnarray*} \frac{\partial\hat{y}_{i}}{\partial y_{i}} &amp; = &amp; \frac{1}{\sqrt{\frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{2}\frac{1}{\left(\frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}+\epsilon\right)^{\frac{3}{2}}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\frac{2}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)\\ &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right) \end{eqnarray*}\] <p>By</p> \[\begin{eqnarray*} \frac{\partial L}{\partial y_{i}} &amp; = &amp; \frac{\partial L}{\partial\hat{y}_{i}}\frac{\partial\hat{y}_{i}}{\partial y_{i}}\\ &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\frac{\partial L}{\partial\hat{y}_{i}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right)\\ &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\left(\frac{\partial L}{\partial\hat{y}_{i}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{i}\hat{y}_{i}^{\top}\right) \end{eqnarray*}\] <p>Note that \(\frac{\partial L}{\partial y_{i}}\) and \(\frac{\partial L}{\partial\hat{y}_{i}}\) are row vectors.</p> <h2 id="derivative-wrt-y-1">Derivative w.r.t \(Y\)</h2> <p>For efficient implementation, it is often more efficient to work with design matrices of size \(n\times d\) where \(n\) is the size of the minibatch, and \(d\) is the feature size. With some algebraic manipulation we write the gradient for all elements in the design matrix:</p> \[\begin{eqnarray*} \nabla_{Y} &amp; = &amp; \left(\begin{array}{ccc} | &amp; &amp; |\\ \left(\frac{\partial L}{\partial y_{1}}\right)^{\top} &amp; &amp; \left(\frac{\partial L}{\partial y_{n}}\right)^{\top}\\ | &amp; &amp; | \end{array}\right)\\ &amp; = &amp; \left(\begin{array}{c} -\,\frac{\partial L}{\partial y_{1}}\,-\\ \vdots\\ -\,\frac{\partial L}{\partial y_{n}}\,- \end{array}\right)^{\top}\\ &amp; = &amp; \left(\begin{array}{c} \frac{1}{\sqrt{\text{var}\left(y_{1}\right)+\epsilon}}\left(\frac{\partial L}{\partial\hat{y}_{1}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\frac{\partial L}{\partial\hat{y}_{1}}\hat{y}_{1}\hat{y}_{1}^{\top}\right)\\ \vdots\\ \frac{1}{\sqrt{\text{var}\left(y_{n}\right)+\epsilon}}\left(\frac{\partial L}{\partial\hat{y}_{n}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\frac{\partial L}{\partial\hat{y}_{n}}\hat{y}_{n}\hat{y}_{n}^{\top}\right) \end{array}\right)^{\top}\\ &amp; = &amp; \left(\left(\begin{array}{c} -\,\frac{\partial L}{\partial\hat{y}_{i}}\,-\\ \vdots\\ -\,\frac{\partial L}{\partial\hat{y}_{i}}\,- \end{array}\right)\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\left(\begin{array}{ccc} \frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{1} &amp; 0 &amp; 0\\ 0 &amp; \ddots &amp; 0\\ 0 &amp; 0 &amp; \frac{\partial L}{\partial\hat{y}_{m}}\hat{y}_{m} \end{array}\right)\left(\begin{array}{c} -\,\hat{y}_{1}^{\top}\,-\\ \vdots\\ -\,\hat{y}_{m}^{\top}\,- \end{array}\right)\right)^{\top}\Sigma_{y}^{-1}\\ &amp; = &amp; \left(\nabla_{\hat{Y}}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-C\hat{Y}^{\top}\right)^{\top}\Sigma_{y}^{-1}\\ &amp; = &amp; \left(\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)\nabla_{\hat{Y}}-\hat{Y}C\right)^{\top}\Sigma_{y}^{-1} \end{eqnarray*}\] <p>we denoted by \(\Sigma_{y}^{-1}=\left(\begin{array}{ccc} \frac{1}{\sqrt{\text{var}\left(y_{1}\right)+\epsilon}}\\ &amp; \ddots\\ &amp; &amp; \frac{1}{\sqrt{\text{var}\left(y_{m}\right)+\epsilon}} \end{array}\right)\) the diagonal matrix of the inverse standard deviation as usually used in BN, and \(C=\frac{1}{n-1}\left(\begin{array}{ccc} \frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{1} &amp; 0 &amp; 0\\ 0 &amp; \ddots &amp; 0\\ 0 &amp; 0 &amp; \frac{\partial L}{\partial\hat{y}_{m}}\hat{y}_{m} \end{array}\right)\) is a diagonal matrix where the coefficients are the (scalar) covariances of the elements of \(\frac{\partial L}{\partial\hat{y}_{i}}\) and \(\hat{y}_{i}\).</p> <h2 id="derivative-wrt-one-line-of-the-weight-matrix">Derivative w.r.t one line of the weight matrix</h2> <p>Using the fact that \(Y=XW^{\top}\), we write \(y_{i}=\left(XW^{\top}\right)_{:i}=Xw_{i}\), where \(w_{i}^{\top}=W_{i:}\) is a line of the weight matrix (that we transpose to obtain a column vector). We can now write the derivative using the chain rule:</p> \[\begin{eqnarray*} \frac{\partial\hat{y}_{i}}{\partial w_{i}} &amp; = &amp; \frac{\partial\hat{y}_{i}}{\partial y_{i}}\frac{\partial y_{i}}{\partial w_{i}}\\ &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right)X \end{eqnarray*}\] \[\begin{eqnarray*} \frac{\partial L}{\partial w_{i}} &amp; = &amp; \frac{\partial L}{\partial\hat{y}_{i}}\frac{\partial\hat{y}_{i}}{\partial w_{i}}\\ &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\frac{\partial L}{\partial\hat{y}_{i}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right)X \end{eqnarray*}\] <h2 id="derivative-wrt-the-whole-matrix">Derivative w.r.t the whole matrix</h2> <p>Now we can stack all lines of the matrix in order to get the derivative for the whole weight matrix:</p> \[\begin{eqnarray*} \nabla_{W} &amp; = &amp; \Sigma_{y}^{-1}\left(\begin{array}{c} \frac{\partial L}{\partial\hat{y}_{1}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{1}\hat{y}_{1}^{\top}\right)X\\ \vdots\\ \frac{\partial L}{\partial\hat{y}_{m}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{m}\hat{y}_{m}^{\top}\right)X \end{array}\right)\\ &amp; = &amp; \Sigma_{y}^{-1}\left(\left(\begin{array}{c} \frac{\partial L}{\partial\hat{y}_{i}}\\ \vdots\\ \frac{\partial L}{\partial\hat{y}_{i}} \end{array}\right)\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)X-\frac{1}{n-1}\left(\begin{array}{ccc} \frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{1} &amp; 0 &amp; 0\\ 0 &amp; \ddots &amp; 0\\ 0 &amp; 0 &amp; \frac{\partial L}{\partial\hat{y}_{m}}\hat{y}_{m} \end{array}\right)\left(\begin{array}{c} \hat{y}_{1}^{\top}\\ \vdots\\ \hat{y}_{m}^{\top} \end{array}\right)X\right)\\ &amp; = &amp; \Sigma_{y}^{-1}\left(\nabla_{\hat{Y}}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-C\hat{Y}^{\top}\right)X \end{eqnarray*}\] <h2 id="derivative-wrt-the-input-of-the-batch-normalized-layer">Derivative w.r.t the input of the batch normalized layer</h2> <p>Using <a class="Reference" href="#eq:bn1">\ref{eq:bn1}</a> and \(Y=XW^{\top}\) we can write \(\nabla_{X}\) using the chain rule:</p> \[\begin{eqnarray*} \nabla_{X} &amp; = &amp; \Sigma_{y}^{-1}\left(\nabla_{\hat{Y}}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-C\hat{Y}^{\top}\right)W^{\top} \end{eqnarray*}\] <h2 id="wrap-up-and-acknowledgements">Wrap-up and acknowledgements</h2> <p>Now you have everything you need !</p> <p>Special thanks to César Laurent for the help and proofreading.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/empirical-fisher/">What is the empirical Fisher ?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/fisher-conditional/">How to compute the Fisher of a conditional when applying natural gradient to neural networks?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/algebra2ndorder/">The algebra of second order methods in neural networks</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/demystifying-natural-nn/">Demystifying Natural Neural Networks</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Thomas George. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-99149577-1"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-99149577-1");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications and pre-prints, also see my google scholar profile",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-software",title:"software",description:"Pieces of code I developed during my research and that I am releasing open-source. For a complete list go to my github page.",section:"Navigation",handler:()=>{window.location.href="/software/"}},{id:"post-derivatives-through-a-batch-norm-layer",title:"Derivatives through a batch norm layer",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/batchnorm-derivatives/"}},{id:"post-what-is-the-empirical-fisher",title:"What is the empirical Fisher ?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/empirical-fisher/"}},{id:"post-how-to-compute-the-fisher-of-a-conditional-when-applying-natural-gradient-to-neural-networks",title:"How to compute the Fisher of a conditional when applying natural gradient to...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/fisher-conditional/"}},{id:"post-the-algebra-of-second-order-methods-in-neural-networks",title:"The algebra of second order methods in neural networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/algebra2ndorder/"}},{id:"post-demystifying-natural-neural-networks",title:"Demystifying Natural Neural Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/demystifying-natural-nn/"}},{id:"news-i-will-give-a-talk-titled-optimization-and-generalization-through-the-lens-of-the-linearization-of-neural-networks-training-dynamics-in-front-of-roger-grosse-https-www-cs-toronto-edu-rgrosse-39-s-group-at-vector-institute-toronto",title:"I will give a talk titled _Optimization and generalization through the lens of...",description:"",section:"News"},{id:"news-i-will-present-nngeometry-https-github-com-tfjgeorge-nngeometry-at-the-pytorch-ecosystem-day-https-pytorch-org-ecosystem-pted-2021",title:"I will present [NNGeometry](https://github.com/tfjgeorge/nngeometry/) at the [Pytorch Ecosystem Day](https://pytorch.org/ecosystem/pted/2021)",description:"",section:"News"},{id:"news-presentation-of-implicit-regularization-via-neural-feature-alignment-at-conf\xe9rence-sur-l-39-apprentissage-automatique-2021-https-cap2021-sciencesconf-org-resource-page-id-8-html",title:"Presentation of _Implicit Regularization via Neural Feature Alignment_ at [Conf\xe9rence sur l&#39;Apprentissage Automatique...",description:"",section:"News"},{id:"news-presentation-of-our-workshop-paper-continual-learning-and-deep-networks-an-analysis-of-the-last-layer-with-timoth\xe9e-lesort-at-the-theory-of-continual-learning-workshop-at-icml",title:"Presentation of our workshop paper *Continual learning and Deep Networks: an Analysis of...",description:"",section:"News"},{id:"news-i-will-present-our-recent-work-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-paper-assets-pdf-lazy-vs-hasty-icml-scis-2022-pdf-poster-assets-pdf-lazy-vs-hasty-icml-scis-2022-poster-pdf-at-the-scis-https-sites-google-com-view-scis-workshop-home-workshop-at-icml-2022",title:"I will present our recent work _Lazy vs hasty: linearization in deep networks...",description:"",section:"News"},{id:"news-new-pre-print-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-https-arxiv-org-abs-2209-09658-available",title:"New pre-print [Lazy vs hasty: linearization in deep networks impacts learning schedule based...",description:"",section:"News"},{id:"news-our-paper-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-https-openreview-net-pdf-id-lukvf4vrfp-was-accepted-to-tmlr-https-openreview-net-forum-id-lukvf4vrfp-code-https-github-com-tfjgeorge-lazy-vs-hasty",title:"Our paper [Lazy vs hasty: linearization in deep networks impacts learning schedule based...",description:"",section:"News"},{id:"news-i-successfully-defended-my-phd-thesis-deep-networks-training-and-generalization-insights-from-linearization-manuscript-assets-pdf-thomas-george-phd-thesis-pdf-slides-assets-pdf-phd-presentation-thomas-george-pdf",title:"I successfully defended my PhD Thesis: **Deep networks training and generalization: insights from...",description:"",section:"News"},{id:"news-i-am-now-a-postdoctoral-researcher-at-orange-labs-https-hellofuture-orange-com-fr-in-vincent-lemaire-http-vincentlemaire-labs-fr-39-s-group",title:"I am now a postdoctoral researcher at [Orange Labs](https://hellofuture.orange.com/fr/) in [Vincent Lemaire](http://vincentlemaire-labs.fr/)&#39;s group....",description:"",section:"News"},{id:"news-i-will-present-our-paper-lazy-vs-hasty-linearization-in-deep-networks-impacts-learning-schedule-based-on-example-difficulty-https-openreview-net-pdf-id-lukvf4vrfp-at-conf\xe9rence-sur-l-39-apprentissage-automatique-cap-https-pfia23-icube-unistra-fr-conferences-cap-index-html-in-strasbourg",title:"I will present our paper [Lazy vs hasty: linearization in deep networks impacts...",description:"",section:"News"},{id:"news-i-joined-orange-innovation-https-hellofuture-orange-com-fr-as-a-permanent-research-scientist-in-the-responsible-and-impactful-ai-program",title:"I joined [Orange Innovation](https://hellofuture.orange.com/fr/) as a permanent research scientist in the Responsible and...",description:"",section:"News"},{id:"news-our-paper-mislabeled-examples-detection-viewed-as-probing-machine-learning-models-concepts-survey-and-extensive-benchmark-https-openreview-net-pdf-id-3ylor7bhkx-was-accepted-to-tmlr-https-openreview-net-forum-id-3ylor7bhkx-code-https-github-com-orange-opensource-mislabeled-video-https-youtu-be-ft9vzxs0nh8",title:"Our paper [Mislabeled examples detection viewed as probing machine learning models: concepts, survey...",description:"",section:"News"},{id:"news-together-with-pierre-nodet-https-scholar-google-com-citations-user-ido8qgeaaaaj-we-will-present-our-recent-works-on-weakly-supervised-learning-at-the-lfi-lip6-seminary-https-lfi-lip6-fr-seminaires-apprentissage-faiblement-supervis\xe9-algorithmes-biqualit\xe9-et-d\xe9tection-automatis\xe9e-d-39-exemples-mal-\xe9tiquet\xe9s",title:"Together with [Pierre Nodet](https://scholar.google.com/citations?user=ido8qGEAAAAJ), we will present our recent works on Weakly supervised...",description:"",section:"News"},{id:"news-i-will-attend-egc-https-www-egc2025-cnrs-fr-in-strasbourg-to-present-our-paper-calibration-des-mod\xe8les-d-apprentissage-pour-l-am\xe9lioration-des-d\xe9tecteurs-automatiques-d-exemples-mal-\xe9tiquet\xe9s-assets-pdf-mislabeled-calibration-paper-fr-pdf-as-well-as-the-workshop-iacd-https-sites-google-com-view-atelier-iacd-2025-for-a-presentation-of-our-paper-mislabeled-examples-detection-viewed-as-probing-machine-learning-models-concepts-survey-and-extensive-benchmark-https-openreview-net-pdf-id-3ylor7bhkx",title:"I will attend [EGC](https://www.egc2025.cnrs.fr/) in Strasbourg to present our paper [Calibration des mod\xe8les...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%74%66%6A%67%65%6F%72%67%65@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=pc3_ujYAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/tfjgeorge","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/tfjgeorge","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/https://bsky.app/profile/tfjgeorge.bsky.social","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>