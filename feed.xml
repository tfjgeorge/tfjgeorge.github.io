<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tfjgeorge.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tfjgeorge.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-05-11T14:13:59+00:00</updated><id>https://tfjgeorge.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">a post with table of contents on a sidebar</title><link href="https://tfjgeorge.github.io/blog/2023/sidebar-table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents on a sidebar" /><published>2023-04-25T14:14:00+00:00</published><updated>2023-04-25T14:14:00+00:00</updated><id>https://tfjgeorge.github.io/blog/2023/sidebar-table-of-contents</id><content type="html" xml:base="https://tfjgeorge.github.io/blog/2023/sidebar-table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents as a sidebar.</p>

<h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2>

<p>To add a table of contents to a post as a sidebar, simply add</p>
<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">sidebar</span><span class="pi">:</span> <span class="s">left</span>
</code></pre></div></div>
<p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code class="language-plaintext highlighter-rouge">left</code> to <code class="language-plaintext highlighter-rouge">right</code>.</p>

<h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h2 data-toc-text="Customizing" id="customizing-your-table-of-contents">Customizing Your Table of Contents</h2>

<p>If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href="https://afeld.github.io/bootstrap-toc/">bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p>

<h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><category term="sidebar" /><summary type="html"><![CDATA[an example of a blog post with table of contents on a sidebar]]></summary></entry><entry><title type="html">displaying beatiful tables with Bootstrap Tables</title><link href="https://tfjgeorge.github.io/blog/2023/tables/" rel="alternate" type="text/html" title="displaying beatiful tables with Bootstrap Tables" /><published>2023-03-20T18:37:00+00:00</published><updated>2023-03-20T18:37:00+00:00</updated><id>https://tfjgeorge.github.io/blog/2023/tables</id><content type="html" xml:base="https://tfjgeorge.github.io/blog/2023/tables/"><![CDATA[<p>Using markdown to display tables is easy. Just use the following syntax:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| Left aligned | Center aligned | Right aligned |
| :----------- | :------------: | ------------: |
| Left 1       | center 1       | right 1       |
| Left 2       | center 2       | right 2       |
| Left 3       | center 3       | right 3       |
</code></pre></div></div>

<p>That will generate:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Left aligned</th>
      <th style="text-align: center">Center aligned</th>
      <th style="text-align: right">Right aligned</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Left 1</td>
      <td style="text-align: center">center 1</td>
      <td style="text-align: right">right 1</td>
    </tr>
    <tr>
      <td style="text-align: left">Left 2</td>
      <td style="text-align: center">center 2</td>
      <td style="text-align: right">right 2</td>
    </tr>
    <tr>
      <td style="text-align: left">Left 3</td>
      <td style="text-align: center">center 3</td>
      <td style="text-align: right">right 3</td>
    </tr>
  </tbody>
</table>

<p></p>

<p>It is also possible to use HTML to display tables. For example, the following HTML code will display a table with <a href="https://bootstrap-table.com/">Bootstrap Table</a>, loaded from a JSON file:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;table</span>
  <span class="na">id=</span><span class="s">"table"</span>
  <span class="na">data-toggle=</span><span class="s">"table"</span>
  <span class="na">data-url=</span><span class="s">"{{ '/assets/json/table_data.json' | relative_url }}"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;thead&gt;</span>
    <span class="nt">&lt;tr&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"id"</span><span class="nt">&gt;</span>ID<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"name"</span><span class="nt">&gt;</span>Item Name<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"price"</span><span class="nt">&gt;</span>Item Price<span class="nt">&lt;/th&gt;</span>
    <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;/thead&gt;</span>
<span class="nt">&lt;/table&gt;</span>
</code></pre></div></div>

<table data-toggle="table" data-url="/assets/json/table_data.json">
  <thead>
    <tr>
      <th data-field="id">ID</th>
      <th data-field="name">Item Name</th>
      <th data-field="price">Item Price</th>
    </tr>
  </thead>
</table>

<p></p>

<p>By using <a href="https://bootstrap-table.com/">Bootstrap Table</a> it is possible to create pretty complex tables, with pagination, search, and more. For example, the following HTML code will display a table, loaded from a JSON file, with pagination, search, checkboxes, and header/content alignment. For more information, check the <a href="https://examples.bootstrap-table.com/index.html">documentation</a>.</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;table</span>
  <span class="na">data-click-to-select=</span><span class="s">"true"</span>
  <span class="na">data-height=</span><span class="s">"460"</span>
  <span class="na">data-pagination=</span><span class="s">"true"</span>
  <span class="na">data-search=</span><span class="s">"true"</span>
  <span class="na">data-toggle=</span><span class="s">"table"</span>
  <span class="na">data-url=</span><span class="s">"{{ '/assets/json/table_data.json' | relative_url }}"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;thead&gt;</span>
    <span class="nt">&lt;tr&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-checkbox=</span><span class="s">"true"</span><span class="nt">&gt;&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"id"</span> <span class="na">data-halign=</span><span class="s">"left"</span> <span class="na">data-align=</span><span class="s">"center"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>ID<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"name"</span> <span class="na">data-halign=</span><span class="s">"center"</span> <span class="na">data-align=</span><span class="s">"right"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>Item Name<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"price"</span> <span class="na">data-halign=</span><span class="s">"right"</span> <span class="na">data-align=</span><span class="s">"left"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>Item Price<span class="nt">&lt;/th&gt;</span>
    <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;/thead&gt;</span>
<span class="nt">&lt;/table&gt;</span>
</code></pre></div></div>

<table data-click-to-select="true" data-height="460" data-pagination="true" data-search="true" data-toggle="table" data-url="/assets/json/table_data.json">
  <thead>
    <tr>
      <th data-checkbox="true"></th>
      <th data-field="id" data-halign="left" data-align="center" data-sortable="true">ID</th>
      <th data-field="name" data-halign="center" data-align="right" data-sortable="true">Item Name</th>
      <th data-field="price" data-halign="right" data-align="left" data-sortable="true">Item Price</th>
    </tr>
  </thead>
</table>]]></content><author><name></name></author><category term="sample-posts" /><summary type="html"><![CDATA[an example of how to use Bootstrap Tables]]></summary></entry><entry><title type="html">a post with table of contents</title><link href="https://tfjgeorge.github.io/blog/2023/table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents" /><published>2023-03-20T15:59:00+00:00</published><updated>2023-03-20T15:59:00+00:00</updated><id>https://tfjgeorge.github.io/blog/2023/table-of-contents</id><content type="html" xml:base="https://tfjgeorge.github.io/blog/2023/table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents in the beginning of the post.</p>

<h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2>

<p>To add a table of contents to a post, simply add</p>
<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">beginning</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div>
<p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post.</p>

<h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h2 id="table-of-contents-options">Table of Contents Options</h2>

<p>If you want to learn more about how to customize the table of contents, you can check the <a href="https://github.com/toshimaru/jekyll-toc">jekyll-toc</a> repository.</p>

<h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><summary type="html"><![CDATA[an example of a blog post with table of contents]]></summary></entry><entry><title type="html">a post with giscus comments</title><link href="https://tfjgeorge.github.io/blog/2022/giscus-comments/" rel="alternate" type="text/html" title="a post with giscus comments" /><published>2022-12-10T15:59:00+00:00</published><updated>2022-12-10T15:59:00+00:00</updated><id>https://tfjgeorge.github.io/blog/2022/giscus-comments</id><content type="html" xml:base="https://tfjgeorge.github.io/blog/2022/giscus-comments/"><![CDATA[<p>This post shows how to add GISCUS comments.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><summary type="html"><![CDATA[an example of a blog post with giscus comments]]></summary></entry><entry><title type="html">a post with redirect</title><link href="https://tfjgeorge.github.io/blog/2022/redirect/" rel="alternate" type="text/html" title="a post with redirect" /><published>2022-02-01T17:39:00+00:00</published><updated>2022-02-01T17:39:00+00:00</updated><id>https://tfjgeorge.github.io/blog/2022/redirect</id><content type="html" xml:base="https://tfjgeorge.github.io/blog/2022/redirect/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[you can also redirect to assets like pdf]]></summary></entry><entry><title type="html">a distill-style blog post</title><link href="https://tfjgeorge.github.io/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post" /><published>2021-05-22T00:00:00+00:00</published><updated>2021-05-22T00:00:00+00:00</updated><id>https://tfjgeorge.github.io/blog/2021/distill</id><content type="html" xml:base="https://tfjgeorge.github.io/blog/2021/distill/"><![CDATA[<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<hr />

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

<hr />

<h2 id="code-blocks">Code Blocks</h2>

<p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags.
An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>.
For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p>

<d-code block="" language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode.
You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="nx">x</span> <span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<hr />

<h2 id="interactive-plots">Interactive Plots</h2>

<p>You can add interative plots using plotly + iframes :framed_picture:</p>

<div class="l-page">
  <iframe src="/assets/plotly/demo.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<p>The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span>
  <span class="s">'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'</span>
<span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
  <span class="n">df</span><span class="p">,</span>
  <span class="n">lat</span><span class="o">=</span><span class="s">'Latitude'</span><span class="p">,</span>
  <span class="n">lon</span><span class="o">=</span><span class="s">'Longitude'</span><span class="p">,</span>
  <span class="n">z</span><span class="o">=</span><span class="s">'Magnitude'</span><span class="p">,</span>
  <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
  <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span>
  <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
  <span class="n">mapbox_style</span><span class="o">=</span><span class="s">"stamen-terrain"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="s">'assets/plotly/demo.html'</span><span class="p">)</span></code></pre></figure>

<hr />

<h2 id="details-boxes">Details boxes</h2>

<p>Details boxes are collapsible boxes which hide additional information from the user. They can be added with the <code class="language-plaintext highlighter-rouge">details</code> liquid tag:</p>

<details><summary>Click here to know more</summary>
<p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p>
</details>

<hr />

<h2 id="layouts">Layouts</h2>

<p>The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p>

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

<p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p>

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

<p>All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:</p>

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

<p>Occasionally you’ll want to use the full browser width.
For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>.
You can also inset the element a little from the edge of the browser by using the inset variant.</p>

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

<p>The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p>

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

<hr />

<h2 id="other-typography">Other Typography?</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>

<ol>
  <li>First ordered list item</li>
  <li>Another item
⋅⋅* Unordered sub-list.</li>
  <li>Actual numbers don’t matter, just that it’s a number
⋅⋅1. Ordered sub-list</li>
  <li>And another item.</li>
</ol>

<p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p>

<p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p>

<ul>
  <li>Unordered list can use asterisks</li>
  <li>Or minuses</li>
  <li>Or pluses</li>
</ul>

<p><a href="https://www.google.com">I’m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I’m a reference-style link</a></p>

<p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here’s our logo (hover to see the title text):</p>

<p>Inline-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1" /></p>

<p>Reference-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2" /></p>

<p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="s">"Python syntax highlighting"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div>

<p>Colons can be used to align columns.</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the
raw Markdown line up prettily. You can also use inline Markdown.</p>

<table>
  <thead>
    <tr>
      <th>Markdown</th>
      <th>Less</th>
      <th>Pretty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Still</em></td>
      <td><code class="language-plaintext highlighter-rouge">renders</code></td>
      <td><strong>nicely</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
  <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p>
</blockquote>

<p>Here’s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry><entry><title type="html">Derivatives through a batch norm layer</title><link href="https://tfjgeorge.github.io/blog/2019/batchnorm-derivatives/" rel="alternate" type="text/html" title="Derivatives through a batch norm layer" /><published>2019-02-15T00:00:00+00:00</published><updated>2019-02-15T00:00:00+00:00</updated><id>https://tfjgeorge.github.io/blog/2019/batchnorm-derivatives</id><content type="html" xml:base="https://tfjgeorge.github.io/blog/2019/batchnorm-derivatives/"><![CDATA[<p>In this note, we will write the derivations for the backpropagated gradient through the batch norm operation, and also the gradient w.r.t the weight matrix. This derivation is very cumbersome and after having tried many different ways (column by column/line by line/element by element/etc), we present the way we think is the simplest one here.</p>

<h2 id="notations">Notations</h2>

<p>We focus on a single batch normalized layer in a fully connected network. The linear part is denoted \(y=Wx\) and parametrized by the weight matrix \(W\). Then the batch normalization operation is computed by:</p>

\[\begin{align*}
\hat{y}=BN\left(y\right) &amp; =\frac{y-\mu_{y}}{\sqrt{\text{var}\left(y\right)+\epsilon}}
\end{align*}\]

<p>Note that we can rewrite it in terms of \(x\) and \(W\) directly instead of computing the intermediate step \(y\):</p>

\[\begin{align}
\hat{y} &amp; =\frac{Wx-W\mu_{x}}{\sqrt{\text{var}\left(Wx\right)+\epsilon}}\nonumber \\
 &amp; =\frac{W\left(x-\mu_{x}\right)}{\sqrt{\text{diag}\left(W^{\top}\text{cov}\left(x\right)W\right)+\epsilon}}\label{eq:bn_wx}
\end{align}\]

<p>Some remarks:</p>

<ol>
<li>These notations are not very precise since we mix up elementwise operations with linear algebra operations. Specifically, by an abuse of notation we divide a vector on the top part of the quotient, by another vector on the bottom part. </li>
<li>Even if we only require to compute an elementwise variance of the components of \(y\), in <a class="Reference" href="#eq:bn_wx">\(\ref{eq:bn_wx}\)</a> we see that it hides the full covariance matrix on the vectors \(x\) in minibatches, here denoted by \(\text{cov}\). It is a dense covariance matrix, with size \(\text{in}\times\text{in}\).</li>
<li>We did not write the scaling and bias parameters \(\gamma\) and \(\beta\) since obtaining their derivative is easier and less interesting.</li>
</ol>

<h2 id="minibatch-vector-notation">Minibatch vector notation</h2>

<p>To clarify things, we consider that the examples are stacked in design matrices of size \(\text{batch size}\times\text{vector size}\):</p>

<p>\(X=\left(\begin{array}{c}
-\,x^{\left(1\right)\top}\,-\\
\vdots\\
-\,x^{\left(n\right)\top}\,-
\end{array}\right)\), \(Y=\left(\begin{array}{c}
-\,y^{\left(1\right)\top}\,-\\
\vdots\\
-\,y^{\left(n\right)\top}\,-
\end{array}\right)\) and \(\hat{Y}=\left(\begin{array}{c}
-\,\hat{y}^{\left(1\right)\top}\,-\\
\vdots\\
-\,\hat{y}^{\left(n\right)\top}\,-
\end{array}\right)\)</p>

<p>Using this notation, we can write the result of BN for a column of the matrix (so all \(i\)s component for all examples in a minibatch). We denote this column by \(y_{i}=Y_{:i}\), as opposed to the lines of \(Y\) that we denoted by \(y^{\left(j\right)\top}=Y_{j:}.\) Note that \(y_{i}\) does not correspond to an example in the minibatch.</p>

<p>We will go step by step:</p>

<p>The mean of a column is obtained by multiplying it with a vector full of \(1\)(denoted by a bold \(\boldsymbol{1}\)), and dividing by \(n\):</p>

\[\begin{eqnarray*}
\frac{1}{n}\sum_{t}\left(y_{i}\right)_{t} &amp; = &amp; \frac{1}{n}\boldsymbol{1}^{\top}y_{i}
\end{eqnarray*}\]

<p>Using this we can write the (unbiased) variance of the column vector \(y_{i}\):</p>

\[\begin{eqnarray*}
\text{var}\left(y_{i}\right) &amp; = &amp; \frac{1}{n-1}\left(y_{i}-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}y_{i}\right)^{\top}\left(y_{i}-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}y_{i}\right)
\end{eqnarray*}\]

<p>We multiplied the mean by a \(\boldsymbol{1}\) vector in order to repeat it along all components of \(y_{i}\). We can simplify the expression:</p>

\[\begin{eqnarray*}
\text{var}\left(y_{i}\right) &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\\
 &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{2}{n}\boldsymbol{1}\boldsymbol{1}^{\top}+\frac{1}{n^{2}}\boldsymbol{1}\boldsymbol{1}^{\top}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\\
 &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{2}{n}\boldsymbol{1}\boldsymbol{1}^{\top}+\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\\
 &amp; = &amp; \frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}
\end{eqnarray*}\]

<p>And so we obtain one column of batch norm:</p>

\[\begin{eqnarray}
\hat{y}_{i} &amp; = &amp; \frac{1}{\sqrt{\frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\label{eq:bn1}
\end{eqnarray}\]

<p>Using this notation we gained the fact that everything here is linear algebra and scalar operations. We do not have any more elementwise operations, nor sums or variance, so it is easier to write derivatives using only elementary calculus rules.</p>

<h2 id="jacobians-and-gradients">Jacobians and gradients</h2>

<p>Writing derivatives of vector functions with respect to vector parameters can be cumbersome, and sometimes ill-defined.</p>

<p>In this note we follow the convention that a gradient of a scalar function of any object has the same shape as this object, so for instance \(\nabla_{W}\) as the same shape as \(W\).</p>

<p>We also make an heavy use of jacobians, which are the matrices of partial derivatives. For a function \(f:\mathbb{R}^{m}\rightarrow\mathbb{R}^{n}\), its jacobian is a \(m\times n\) matrix, defined by:</p>

\[\begin{eqnarray*}
\left(\frac{\partial f\left(x\right)}{\partial x}\right)_{ij} &amp; = &amp; \frac{\partial f\left(x\right)_{i}}{\partial x_{j}}
\end{eqnarray*}\]

<p>Using this notation the chain rule can be written for a composition of function \(f=g\circ h\):</p>

\[\frac{\partial f\left(x\right)}{\partial x}=\frac{\partial g\left(h\left(x\right)\right)}{\partial x}=\frac{\partial g\left(h\right)}{\partial h}\frac{\partial h\left(x\right)}{\partial x}\]

<p>Using this notation it is also easy to write first order Taylor series expansion of vector functions. The first order term is just the jacobian matrix, that we can multiply to the right by an increment \(dx\):</p>

\[f\left(x+dx\right)=f\left(x\right)+\frac{\partial f\left(x\right)}{\partial x}dx+o\left(dx\right)\]

<p>Since \(\frac{\partial f\left(x\right)}{\partial x}\) is a \(m\times n\) matrix then \(\frac{\partial f\left(x\right)}{\partial x}dx\) is a \(m\times1\) column vector so it lives in the same space as \(f\). Everything works out fine!</p>

<h2 id="derivative-wrt-y">Derivative w.r.t \(y\)</h2>

<p>We start by computing the derivative through the BN operation. One of the weakness of BN is that each batch normalized feature will be a function of all other elements in a minibatch, because of the mean and variance. This is why we will focus on a single column of the design matrix \(\hat{Y}\): in this case all elements of this column only depend on the elements of the corresponding column in \(\hat{Y}\).</p>

<p>We write the derivative using the expression in <a class="Reference" href="#eq:bn1">\ref{eq:bn1}</a>.</p>

\[\begin{eqnarray*}
\frac{\partial\hat{y}_{i}}{\partial y_{i}} &amp; = &amp; \frac{1}{\sqrt{\frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{2}\frac{1}{\left(\frac{1}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}+\epsilon\right)^{\frac{3}{2}}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)y_{i}\frac{2}{n-1}y_{i}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)\\
 &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right)
\end{eqnarray*}\]

<p>By</p>

\[\begin{eqnarray*}
\frac{\partial L}{\partial y_{i}} &amp; = &amp; \frac{\partial L}{\partial\hat{y}_{i}}\frac{\partial\hat{y}_{i}}{\partial y_{i}}\\
 &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\frac{\partial L}{\partial\hat{y}_{i}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right)\\
 &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\left(\frac{\partial L}{\partial\hat{y}_{i}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{i}\hat{y}_{i}^{\top}\right)
\end{eqnarray*}\]

<p>Note that \(\frac{\partial L}{\partial y_{i}}\) and \(\frac{\partial L}{\partial\hat{y}_{i}}\) are row vectors.</p>

<h2 id="derivative-wrt-y-1">Derivative w.r.t \(Y\)</h2>

<p>For efficient implementation, it is often more efficient to work with design matrices of size \(n\times d\) where \(n\) is the size of the minibatch, and \(d\) is the feature size. With some algebraic manipulation we write the gradient for all elements in the design matrix:</p>

\[\begin{eqnarray*}
\nabla_{Y} &amp; = &amp; \left(\begin{array}{ccc}
| &amp;  &amp; |\\
\left(\frac{\partial L}{\partial y_{1}}\right)^{\top} &amp;  &amp; \left(\frac{\partial L}{\partial y_{n}}\right)^{\top}\\
| &amp;  &amp; |
\end{array}\right)\\
 &amp; = &amp; \left(\begin{array}{c}
-\,\frac{\partial L}{\partial y_{1}}\,-\\
\vdots\\
-\,\frac{\partial L}{\partial y_{n}}\,-
\end{array}\right)^{\top}\\
 &amp; = &amp; \left(\begin{array}{c}
\frac{1}{\sqrt{\text{var}\left(y_{1}\right)+\epsilon}}\left(\frac{\partial L}{\partial\hat{y}_{1}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\frac{\partial L}{\partial\hat{y}_{1}}\hat{y}_{1}\hat{y}_{1}^{\top}\right)\\
\vdots\\
\frac{1}{\sqrt{\text{var}\left(y_{n}\right)+\epsilon}}\left(\frac{\partial L}{\partial\hat{y}_{n}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\frac{\partial L}{\partial\hat{y}_{n}}\hat{y}_{n}\hat{y}_{n}^{\top}\right)
\end{array}\right)^{\top}\\
 &amp; = &amp; \left(\left(\begin{array}{c}
-\,\frac{\partial L}{\partial\hat{y}_{i}}\,-\\
\vdots\\
-\,\frac{\partial L}{\partial\hat{y}_{i}}\,-
\end{array}\right)\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-\frac{1}{n-1}\left(\begin{array}{ccc}
\frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{1} &amp; 0 &amp; 0\\
0 &amp; \ddots &amp; 0\\
0 &amp; 0 &amp; \frac{\partial L}{\partial\hat{y}_{m}}\hat{y}_{m}
\end{array}\right)\left(\begin{array}{c}
-\,\hat{y}_{1}^{\top}\,-\\
\vdots\\
-\,\hat{y}_{m}^{\top}\,-
\end{array}\right)\right)^{\top}\Sigma_{y}^{-1}\\
 &amp; = &amp; \left(\nabla_{\hat{Y}}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-C\hat{Y}^{\top}\right)^{\top}\Sigma_{y}^{-1}\\
 &amp; = &amp; \left(\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)\nabla_{\hat{Y}}-\hat{Y}C\right)^{\top}\Sigma_{y}^{-1}
\end{eqnarray*}\]

<p>we denoted by \(\Sigma_{y}^{-1}=\left(\begin{array}{ccc}
\frac{1}{\sqrt{\text{var}\left(y_{1}\right)+\epsilon}}\\
 &amp; \ddots\\
 &amp;  &amp; \frac{1}{\sqrt{\text{var}\left(y_{m}\right)+\epsilon}}
\end{array}\right)\) the diagonal matrix of the inverse standard deviation as usually used in BN, and \(C=\frac{1}{n-1}\left(\begin{array}{ccc}
\frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{1} &amp; 0 &amp; 0\\
0 &amp; \ddots &amp; 0\\
0 &amp; 0 &amp; \frac{\partial L}{\partial\hat{y}_{m}}\hat{y}_{m}
\end{array}\right)\) is a diagonal matrix where the coefficients are the (scalar) covariances of the elements of \(\frac{\partial L}{\partial\hat{y}_{i}}\) and \(\hat{y}_{i}\).</p>

<h2 id="derivative-wrt-one-line-of-the-weight-matrix">Derivative w.r.t one line of the weight matrix</h2>

<p>Using the fact that \(Y=XW^{\top}\), we write \(y_{i}=\left(XW^{\top}\right)_{:i}=Xw_{i}\), where \(w_{i}^{\top}=W_{i:}\) is a line of the weight matrix (that we transpose to obtain a column vector). We can now write the derivative using the chain rule:</p>

\[\begin{eqnarray*}
\frac{\partial\hat{y}_{i}}{\partial w_{i}} &amp; = &amp; \frac{\partial\hat{y}_{i}}{\partial y_{i}}\frac{\partial y_{i}}{\partial w_{i}}\\
 &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right)X
\end{eqnarray*}\]

\[\begin{eqnarray*}
\frac{\partial L}{\partial w_{i}} &amp; = &amp; \frac{\partial L}{\partial\hat{y}_{i}}\frac{\partial\hat{y}_{i}}{\partial w_{i}}\\
 &amp; = &amp; \frac{1}{\sqrt{\text{var}\left(y_{i}\right)+\epsilon}}\frac{\partial L}{\partial\hat{y}_{i}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{i}\hat{y}_{i}^{\top}\right)X
\end{eqnarray*}\]

<h2 id="derivative-wrt-the-whole-matrix">Derivative w.r.t the whole matrix</h2>

<p>Now we can stack all lines of the matrix in order to get the derivative for the whole weight matrix:</p>

\[\begin{eqnarray*}
\nabla_{W} &amp; = &amp; \Sigma_{y}^{-1}\left(\begin{array}{c}
\frac{\partial L}{\partial\hat{y}_{1}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{1}\hat{y}_{1}^{\top}\right)X\\
\vdots\\
\frac{\partial L}{\partial\hat{y}_{m}}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}-\frac{1}{n-1}\hat{y}_{m}\hat{y}_{m}^{\top}\right)X
\end{array}\right)\\
 &amp; = &amp; \Sigma_{y}^{-1}\left(\left(\begin{array}{c}
\frac{\partial L}{\partial\hat{y}_{i}}\\
\vdots\\
\frac{\partial L}{\partial\hat{y}_{i}}
\end{array}\right)\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)X-\frac{1}{n-1}\left(\begin{array}{ccc}
\frac{\partial L}{\partial\hat{y}_{i}}\hat{y}_{1} &amp; 0 &amp; 0\\
0 &amp; \ddots &amp; 0\\
0 &amp; 0 &amp; \frac{\partial L}{\partial\hat{y}_{m}}\hat{y}_{m}
\end{array}\right)\left(\begin{array}{c}
\hat{y}_{1}^{\top}\\
\vdots\\
\hat{y}_{m}^{\top}
\end{array}\right)X\right)\\
 &amp; = &amp; \Sigma_{y}^{-1}\left(\nabla_{\hat{Y}}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-C\hat{Y}^{\top}\right)X
\end{eqnarray*}\]

<h2 id="derivative-wrt-the-input-of-the-batch-normalized-layer">Derivative w.r.t the input of the batch normalized layer</h2>

<p>Using <a class="Reference" href="#eq:bn1">\ref{eq:bn1}</a> and \(Y=XW^{\top}\) we can write \(\nabla_{X}\) using the chain rule:</p>

\[\begin{eqnarray*}
\nabla_{X} &amp; = &amp; \Sigma_{y}^{-1}\left(\nabla_{\hat{Y}}^{\top}\left(I-\frac{1}{n}\boldsymbol{1}\boldsymbol{1}^{\top}\right)-C\hat{Y}^{\top}\right)W^{\top}
\end{eqnarray*}\]

<h2 id="wrap-up-and-acknowledgements">Wrap-up and acknowledgements</h2>

<p>Now you have everything you need !</p>

<p>Special thanks to César Laurent for the help and proofreading.</p>]]></content><author><name></name></author><category term="note" /><summary type="html"><![CDATA[In this note, we will write the derivations for the backpropagated gradient through the batch norm operation, and also the gradient w.r.t the weight matrix. This derivation is very cumbersome and after having tried many different ways (column by column/line by line/element by element/etc), we present the way we think is the simplest one here.]]></summary></entry><entry><title type="html">What is the empirical Fisher ?</title><link href="https://tfjgeorge.github.io/blog/2018/empirical-fisher/" rel="alternate" type="text/html" title="What is the empirical Fisher ?" /><published>2018-11-09T00:00:00+00:00</published><updated>2018-11-09T00:00:00+00:00</updated><id>https://tfjgeorge.github.io/blog/2018/empirical-fisher</id><content type="html" xml:base="https://tfjgeorge.github.io/blog/2018/empirical-fisher/"><![CDATA[<p>Some recent papers mention that they use the inverse of the “empirical Fisher” as a preconditioner. The main reason is its simplicity of use since it only requires gradients of the loss with respect to the parameters for each individual example. These are the same gradients as the ones we use to estimate our expected gradient when using SGD, as opposed to the true Fisher used in natural gradient, where the gradients that we need are gradients sampled from the distribution represented by our neural network.</p>

<p>The update using the “empirical Fisher” is:</p>

\[\begin{eqnarray*}
\theta &amp; \leftarrow &amp; \theta-\eta\left(\underbrace{\mathbb{E}\left[\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\left(\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\right)^{\top}\right]}_{C}+\epsilon\mathbf{I}\right)^{-1}\underbrace{\mathbb{E}\left[\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\right]}_{g}
\end{eqnarray*}\]

<p>Where \(g\) is often estimated using its minibatch estimate, and \(C\) is the (uncentered) covariance of the gradients, also estimated using a minibatch, or a running average. \(\eta\) is the learning rate, and \(\epsilon\) is a Tikhonov damping parameter.</p>

<h2 id="what-problem-are-we-solving-when-using-this-update">What problem are we solving when using this update?</h2>

<p><b>Claim</b>: This update is solution to the following problem, up to a second order approximation:</p>

\[\text{min}_{\Delta\theta}L\left(\theta+\Delta\theta\right)\text{ such that }\mathbb{E}\left[\left(\Delta\ell\left(x,\theta\right)\right)^{2}\right]=c\]

<p>Where we defined \(\Delta\ell\left(x,\theta\right)=\ell\left(x,\theta+\Delta\theta\right)-\ell\left(x,\theta\right)\), and \(c\) is a predefined scalar constant.</p>

<p><i>Proof: </i>We start by writing the first order Taylor series expansion of \(\ell\left(x,\theta+\Delta\theta\right)\):</p>

\[\begin{eqnarray*}
\Delta\ell\left(x,\theta\right) &amp; = &amp; \left(\ell\left(x,\theta\right)+\left(\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\right)^{\top}\Delta\theta+o\left(\left\Vert \Delta\theta\right\Vert _{2}\right)\right)-\ell\left(x,\theta\right)\\
 &amp; = &amp; \left(\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\right)^{\top}\Delta\theta+o\left(\left\Vert \Delta\theta\right\Vert _{2}\right)
\end{eqnarray*}\]

<p>Where \(o\left(\left\Vert \Delta\theta\right\Vert _{2}\right)\) hides the higher order terms. It is a function such that \(\lim_{x\rightarrow0}\frac{o\left(x\right)}{x}=0\), or to put it into words, it will be negligible compared to the first order term as long as \(\left\Vert \Delta\theta\right\Vert _{2}\) is not too big.</p>

<p>By replacing in the constraint we obtain:</p>

\[\begin{eqnarray*}
\mathbb{E}\left[\left(\Delta\ell\left(x,\theta\right)\right)^{2}\right] &amp; = &amp; \mathbb{E}\left[\left(\left(\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\right)^{\top}\Delta\theta+o\left(\left\Vert \Delta\theta\right\Vert _{2}\right)\right)^{2}\right]\\
 &amp; = &amp; \mathbb{E}\left[\left(\left(\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\right)^{\top}\Delta\theta\right)^{2}\right]+o\left(\left\Vert \Delta\theta\right\Vert _{2}^{2}\right)
\end{eqnarray*}\]

<p>In the second line we have hidden the cross product in \(o\left(\left\Vert \Delta\theta\right\Vert _{2}^{2}\right)\).</p>

<p>We now remark that we can rewrite:</p>

\[\begin{eqnarray*}
\mathbb{E}\left[\left(\left(\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\right)^{\top}\Delta\theta\right)^{2}\right] &amp; = &amp; \mathbb{E}\left[\left(\Delta\theta^{\top}\left(\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\right)\right)\left(\left(\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\right)^{\top}\Delta\theta\right)\right]\\
 &amp; = &amp; \Delta\theta^{\top}\mathbb{E}\left[\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\left(\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\right)^{\top}\right]\Delta\theta\\
 &amp; = &amp; \Delta\theta^{\top}C\Delta\theta
\end{eqnarray*}\]

<p>And so our minimization problem becomes:</p>

\[\text{min}_{\Delta\theta}L\left(\theta+\Delta\theta\right)\text{ such that }\Delta\theta^{\top}C\Delta\theta=c\]

<p>Which can be solved e.g. using Lagrange multipliers, and we obtain the update:</p>

\[\begin{eqnarray*}
\Delta\theta^{*} &amp; = &amp; -\eta\left(C+\epsilon\mathbf{I}\right)^{-1}\mathbb{E}\left[\frac{\partial\ell\left(x,\theta\right)}{\partial\theta}\right]
\end{eqnarray*}\]

<p>Where \(\eta\) is a scalar that we usually define as being the (constant) learning rate, but to be more precise it should be set so that the constraint \(\Delta\theta^{\top}C\Delta\theta=c\) is enforced. The role of \(\epsilon\) is to make sure that regardless of the spectrum of \(C\), the update will not get too big, and make our second order approximation wrong.</p>

<h2 id="discussion">Discussion</h2>

<p>What does it mean to be solving this minimization problem?</p>

\[\text{min}_{\Delta\theta}L\left(\theta+\Delta\theta\right)\text{ such that }\mathbb{E}\left[\left(\Delta\ell\left(x,\theta\right)\right)^{2}\right]=c\]

<p>First, it means that we measure progress in the space of our loss function. It has the desirable effect of making this update invariant by reparametrization of the network, as long as \(\epsilon\) is kept small.</p>

<p>Second, it means that we will encourage all examples to have their loss reduced by a similar amount, on average \(\sqrt{c}\). Is this something desirable or not ? I don’t know but I am open to your suggestions!</p>]]></content><author><name></name></author><category term="note" /><summary type="html"><![CDATA[Some recent papers mention that they use the inverse of the “empirical Fisher” as a preconditioner. The main reason is its simplicity of use since it only requires gradients of the loss with respect to the parameters for each individual example. These are the same gradients as the ones we use to estimate our expected gradient when using SGD, as opposed to the true Fisher used in natural gradient, where the gradients that we need are gradients sampled from the distribution represented by our neural network.]]></summary></entry><entry><title type="html">How to compute the Fisher of a conditional when applying natural gradient to neural networks?</title><link href="https://tfjgeorge.github.io/blog/2018/fisher-conditional/" rel="alternate" type="text/html" title="How to compute the Fisher of a conditional when applying natural gradient to neural networks?" /><published>2018-10-29T00:00:00+00:00</published><updated>2018-10-29T00:00:00+00:00</updated><id>https://tfjgeorge.github.io/blog/2018/fisher-conditional</id><content type="html" xml:base="https://tfjgeorge.github.io/blog/2018/fisher-conditional/"><![CDATA[<p>This short note aims at explaining how we come up with an expression for the Fisher Information Matrix in the context of the conditional distributions represented by neural networks.</p>

<p>In neural networks, the so called natural gradient is a preconditioner for the gradient descent algorithm, where the update is regularized so that each update \(\Delta\theta\) of the values of the parameters \(\theta\) will be measured using the \(KL\) divergence. This has some interesting properties, such as the effect of making the update invariant to reparametrization of our neural network: more explanation to come in another blog post. The update is given by:</p>

\[\begin{eqnarray*}
\Delta_{nat}\theta &amp; = &amp; -\mathbf{F}_{\theta}^{-1}\mathbb{E}_{\left(x,y\right)\sim\mathcal{D}_{train}}\left[\nabla_{\theta}\left\{ -\log p_{\theta}\left(y|x\right)\right\} \right]
\end{eqnarray*}\]

<p>where:</p>

<ul>
<li>
the expectation is taken using (discrete) samples \(\left(x,y\right)\) of the training set \(\mathcal{D}_{train}\);
</li>
<li>
\(p_{\theta}\left(y|x\right)\) is our neural network with \(x\) the input (e.g. the pixels of an image), and \(y\) the output (e.g. the 10 coefficients of the softmax for MNIST where we have 10 classes = 10 digits);
</li>
<li>
we use the negative log likelihood as our loss function \(-\log p_{\theta}\left(y|x\right)\), and so \(\nabla_{\theta}\left\{ -\log p_{\theta}\left(y|x\right)\right\} \) is the gradient of our loss with respect to the parameters \(\theta\);
</li>
<li>
\(\mathbf{F}_{\theta}\) is the Fisher Information Matrix (FIM) , defined as:
</li>

</ul>

\[\begin{eqnarray*}
\mathbf{F}_{\theta} &amp; = &amp; \mathbb{E}_{z\sim p_{\theta}\left(z\right)}\left[\frac{\partial\log p_{\theta}\left(z\right)}{\partial\theta}\left(\frac{\partial\log p_{\theta}\left(z\right)}{\partial\theta}\right)^{\top}\right]
\end{eqnarray*}\]

<p>The link between the \(KL\) and the FIM resides in the fact that the FIM is the second order term of the Taylor series expansion of the \(KL\): For a distribution \(p_{\theta}\left(z\right)\) it is given by:</p>

\[\begin{eqnarray*}
KL\left(p_{\theta}\left(z\right)\parallel p_{\theta+\Delta\theta}\left(z\right)\right) &amp; = &amp; \Delta\theta^{\top}\mathbf{F}_{\theta}\Delta\theta+o\left(\left\Vert \Delta\theta\right\Vert _{2}^{2}\right)
\end{eqnarray*}\]

<p>where \(o\left(\left\Vert \Delta\theta\right\Vert _{2}^{2}\right)\) is negligible compared to \(\Delta\theta^{\top}\mathbf{F}\Delta\theta\) when \(\left\Vert \Delta\theta\right\Vert _{2}\) is small, the first order term is \(0\).</p>

<p>This is the general definition for \(\mathbf{F}_{\theta}\), using a density \(p_{\theta}\left(z\right)\). But when applying this technique to train neural networks, we model the conditional \(p_{\theta}\left(y\vert x\right)\). So how do we apply this to neural networks training, i.e. for the conditional \(p_{\theta}\left(y\vert x\right)\)?</p>

<p>Here is my explanation.</p>

<p>Instead of just considering \(p_{\theta}\left(y\vert x\right)\) we will use the joint probability \(p_{\theta}\left(y,x\right)=p_{\theta}\left(y\vert x\right)p\left(x\right)\). We have introduced \(p\left(x\right)\) which is the distribution over the inputs. If the task is image classification, this is the distribution of the natural images \(x\). Usually we do not have access to \(p\left(x\right)\) explicitely, but instead we have samples from it, which are the images in our training set.</p>

<p>By replacing \(p_{\theta}\left(z\right)\) with \(p_{\theta}\left(x,y\right)\) in the formula above, we can consider \(KL\left(p_{\theta}\left(x,y\right)\parallel p_{\theta+\Delta\theta}\left(x,y\right)\right)\) and write the FIM for this joint distribution:</p>

\[\begin{eqnarray*}
\mathbf{F}_{\theta} &amp; = &amp; \mathbb{E}_{\left(x,y\right)\sim p_{\theta}\left(x,y\right)}\left[\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\left(\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta}\right)^{\top}\right]
\end{eqnarray*}\]

<p>Next we replace the joint with the product of the marginal over \(x\) and the conditional in the derivative:</p>

\[\begin{eqnarray*}
\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta} &amp; = &amp; \frac{\partial\log\left(p_{\theta}\left(y|x\right)p\left(x\right)\right)}{\partial\theta}\\
 &amp; = &amp; \frac{\partial\left(\log p_{\theta}\left(y|x\right)+\log p\left(x\right)\right)}{\partial\theta}\\
 &amp; = &amp; \frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}+\frac{\partial\log p\left(x\right)}{\partial\theta}
\end{eqnarray*}\]

<p>and since \(p\left(x\right)\) does not depend on \(\theta\) then \(\frac{\partial\log p\left(x\right)}{\partial\theta}=0\). This simplifies in:</p>

\[\begin{eqnarray*}
\frac{\partial\log p_{\theta}\left(x,y\right)}{\partial\theta} &amp; = &amp; \frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}
\end{eqnarray*}\]

<p>Equivalently for the expectation, we can take the expectation in 2 steps:</p>

<ol>
<li>
sample a \(x\) from our training distribution;
</li>
<li>
for this value of \(x\) compute \(p_{\theta}\left(y|x\right)\) then sample multiple points to estimate the expectation over \(p_{\theta}\left(y|x\right)\). Here we also require multiple backprops to compute the gradients for each sample \(y\).
</li>

</ol>

<p>Finally we get the desired formula:</p>

\[\begin{eqnarray*}
\mathbf{F}_{\theta} &amp; = &amp; \mathbb{E}_{x\sim p\left(x\right),y\sim p_{\theta}\left(y|x\right)}\left[\frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}\left(\frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}\right)^{\top}\right]\\
 &amp; = &amp; \mathbb{E}_{x\sim p\left(x\right)}\left[\mathbb{E}_{y\sim p_{\theta}\left(y|x\right)}\left[\frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}\left(\frac{\partial\log p_{\theta}\left(y|x\right)}{\partial\theta}\right)^{\top}\right]\right]
\end{eqnarray*}\]

<p>And so we get the FIM for a conditional distribution.</p>]]></content><author><name></name></author><category term="note" /><summary type="html"><![CDATA[This short note aims at explaining how we come up with an expression for the Fisher Information Matrix in the context of the conditional distributions represented by neural networks.]]></summary></entry><entry><title type="html">The algebra of second order methods in neural networks</title><link href="https://tfjgeorge.github.io/blog/2017/algebra2ndorder/" rel="alternate" type="text/html" title="The algebra of second order methods in neural networks" /><published>2017-10-29T00:00:00+00:00</published><updated>2017-10-29T00:00:00+00:00</updated><id>https://tfjgeorge.github.io/blog/2017/algebra2ndorder</id><content type="html" xml:base="https://tfjgeorge.github.io/blog/2017/algebra2ndorder/"><![CDATA[<p>This note gives the derivations for the inverse of 2 different but related 2nd order matrices: the Fisher Information Matrix, and the Gauss-Newton approximation of the Hessian. In particular we highlight 2 centering properties that follow from the local structure of those matrices:</p>

<ul>
<li>
we should always use a centered update for weight matrices, even if it does not follow the gradient direction (see section <a class="Reference" href="#subsec:Updating-the-weight">4.2↓</a>)
</li>
<li>
we should normalize using the (centered) covariance matrix of the activation of each layer (see section <a class="Reference" href="#subsec:KFAC-inversion">3.2↓</a>)
</li>

</ul>

<p>Along the way, we describe the derivation of an approximate method using the properties of the Kronecker product known as KFAC <span class="bibcites">[<a class="bibliocite" name="cite-2" href="#biblio-2"><span class="bib-index">2</span></a>]</span> with corresponding parameter updates, and we give a motivation for the centering trick used in Natural Neural Networks <span class="bibcites">[<a class="bibliocite" name="cite-1" href="#biblio-1"><span class="bib-index">1</span></a>]</span>.</p>

<h2 id="notations-and-problem-statement">Notations and problem statement</h2>

<p>We denote by \(f\left(x;\theta\right)\) the output of a fully connected neural network parametrized by its weight matrices and bias vectors grouped in a vector of paramters \(\theta\). Let us denote by \(\ell\left(f\left(x;\theta\right),y\left(x\right)\right)\) a loss function between the value given by the model \(f\left(x;\theta\right)\) and the true value \(y\left(x\right)\). \(\mathcal{L}\) is the empirical risk on a train set of \(n\) examples: \(\mathcal{L}\left(\theta\right)=\frac{1}{n}\sum_{i}\ell\left(f\left(x_{i};\theta\right),y\left(x_{i}\right)\right)=\frac{1}{n}\sum_{i}\ell\left(x_{i}\right)\) where we denoted \(\ell\left(x_{i}\right)=\ell\left(f\left(x_{i};\theta\right),y\left(x_{i}\right)\right)\) to simplify notations.</p>

<p>Suppose a second order update \(\theta^{t+1}=\theta^{t}-\lambda G^{-1}\nabla_{\theta}\mathcal{L}\). \(G\) can be the Hessian matrix or an approximation given by Gauss Newton. \(G\) can also be the Fisher Information Matrix and in this case the update is called the natural gradient. By writing the expression for Gauss-Newton and Fisher, we observe that they share a similar structure:</p>

\[\begin{eqnarray*}
GN &amp; = &amp; \mathbb{E}_{p\left(x\right)}\left[J^{T}\frac{\partial^{2}\ell\left(x\right)}{\partial f^{2}}J\right]
\end{eqnarray*}\]

\[\begin{eqnarray*}
F &amp; = &amp; \mathbb{E}_{p\left(x\right)}\left[J^{T}D\left(x\right)J\right]
\end{eqnarray*}\]

<p>\(J=\frac{\partial f\left(x;\theta\right)}{\partial\theta}\) is the jacobian matrix of the output of the network, with respect to the parameters \(\theta\). It is of size \(n_{output}\times n_{parameters}\). For a small change \(\Delta\theta\) it is a first order measure of the change in the value of \(f\left(x\right)\) or more precisely \(f\left(x;\theta+\Delta\theta\right)\approx f\left(x;\theta\right)+J\Delta\theta\). The expression for the Fisher Information Matrix is given by <span class="bibcites">[<a class="bibliocite" name="cite-3" href="#biblio-3"><span class="bib-index">3</span></a>]</span>. Without loss of generality we denote both matrices by:\(\begin{eqnarray*}
G &amp; = &amp; \mathbb{E}\left[J^{T}DJ\right]
\end{eqnarray*}\)</p>

<h2 id="local-expression-for-the-matrix">Local expression for the matrix</h2>

<p>The matrix \(G\) has size \(n_{parameters}\times n_{parameters}\). For a typical neural network with several millions of parameters it is untractable to store and to invert. We usually approximate it as block diagonal, where each block is a square matrix of the size of the number of scalar parameter values for a layer. With this structure, we can invert each block separately and apply the update layer by layer: \(\theta_{l}^{t+1}=\theta_{l}^{t}-\lambda G_{l}^{-1}\nabla_{\theta_{l}}\mathcal{L}\). Let us now give an exact expression for this smaller matrix \(G_{l}\) and its inverse \(G_{l}^{-1}\). We call it local it the sense that it is local to a layer.</p>

<h3 id="stacking-the-parameters">Stacking the parameters</h3>

<p>The computation made by a layer is given by \(h_{l+1}=f_{l}\left(W_{l}h_{l}+b_{l}\right)=f_{l}\left(a_{l}\right)\). The parameters for this layer are a matrix \(W_{l}\) and a vector\(b_{l}\). But in order to write a concise expression for \(G_{l}\) we need to stack them into a vector \(\theta_{l}\) so that the gradient \(\nabla_{\theta_{l}}\mathcal{L}\) is a vector and writing \(G_{l}^{-1}\nabla_{\theta_{l}}\mathcal{L}\) makes sense. To this end, we use the operator \(vec\) that stacks the column of a matrix into a vector, i.e. for a \(2\times2\) matrix:</p>

\[\begin{align*}
A= &amp; \left(\begin{array}{cc}
A_{11} &amp; A_{12}\\
A_{21} &amp; A_{22}
\end{array}\right) &amp; vec\left(A\right)= &amp; \left(\begin{array}{c}
A_{11}\\
A_{21}\\
A_{12}\\
A_{22}
\end{array}\right)
\end{align*}\]

<p>Our full vector of parameters becomes:</p>

\[\begin{eqnarray*}
\theta_{l} &amp; = &amp; \left(\begin{array}{c}
vec\left(W\right)\\
b
\end{array}\right)
\end{eqnarray*}\]

<h3 id="expressions-for-the-jacobians-">Expressions for the jacobians<a class="Label" name="subsec:Expressions-for-the"> </a></h3>

<p>We now focus on the block \(G_{l}=\mathbb{E}\left[J_{l}^{T}DJ_{l}\right]\) for layer \(l\). We require an expression for \(J_{l}=\frac{\partial f\left(x;\theta\right)}{\partial\theta_{l}}\). By the chain rule we separate it into a back propagated contribution \(J_{a_{l}}\) and a local contribution:</p>

\[\begin{eqnarray*}
J_{l} &amp; = &amp; \frac{\partial f\left(x;\theta\right)}{\partial a_{l}}\frac{\partial a_{l}}{\partial\theta_{l}}\\
 &amp; = &amp; J_{a_{l}}\frac{\partial a_{l}}{\partial\theta_{l}}
\end{eqnarray*}\]

<p>To obtain an exact expression for \(\frac{\partial a_{l}}{\partial\theta_{l}}\) we will use \(vec\) once again with the property that \(vec\left(AXB\right)=\left(B^{T}\otimes A\right)vec\left(X\right)\) where \(\otimes\) is the Kronecker product:</p>

\[\begin{eqnarray}
a_{l} &amp; = &amp; W_{l}h_{l}+b_{l}\nonumber \\
 &amp; = &amp; vec\left(W_{l}h_{l}\right)+b_{l}\nonumber \\
 &amp; = &amp; vec\left(\mathbf{I}W_{l}h_{l}\right)+b_{l}\\
 &amp; = &amp; \left(h_{l}^{T}\otimes\mathbf{I}\right)vec\left(W_{l}\right)+b_{l}\label{eq:flattened_linear}
\end{eqnarray}\]

<p>In the second line we used the fact that \(W_{l}h_{l}\) is a vector and thus \(vec\left(W_{l}h_{l}\right)=W_{l}h_{l}\). We also introduced \(\mathbf{I}\) the identity matrix of the same size of \(a_{l}\). From eq <a class="Reference" href="#eq:flattened_linear">\ref{eq:flattened_linear}</a> we can directly read the jacobians:</p>

\[\begin{eqnarray*}
\frac{\partial a_{l}}{\partial vec\left(W_{l}\right)} &amp; = &amp; \left(h_{l}^{T}\otimes\mathbf{I}\right)\\
\frac{\partial a_{l}}{\partial b_{l}} &amp; = &amp; \mathbf{I}
\end{eqnarray*}\]

<p>Now using \(\theta_{l}\):</p>

\[\begin{eqnarray*}
\frac{\partial a_{l}}{\partial\theta_{l}} &amp; = &amp; \left(\left(\begin{array}{cc}
h_{l}^{T} &amp; 1\end{array}\right)\otimes\mathbf{I}\right)
\end{eqnarray*}\]

<h3 id="expression-for-the-block">Expression for the block</h3>

<p>Getting back to the block \(G_{l}=\mathbb{E}\left[J_{l}^{T}DJ_{l}\right]\) we get a simple expression:</p>

\[\begin{eqnarray}
G_{l} &amp; = &amp; \mathbb{E}\left[\left(\frac{\partial a_{l}}{\partial\theta_{l}}\right)^{T}J_{a_{l}}^{T}DJ_{a_{l}}\frac{\partial a_{l}}{\partial\theta_{l}}\right]\nonumber \\
 &amp; = &amp; \mathbb{E}\left[\left(\left(\begin{array}{cc}
h_{l}^{T} &amp; 1\end{array}\right)\otimes\mathbf{I}\right)^{T}J_{a_{l}}^{T}DJ_{a_{l}}\left(\left(\begin{array}{cc}
h_{l}^{T} &amp; 1\end{array}\right)\otimes\mathbf{I}\right)\right]\nonumber \\
 &amp; = &amp; \mathbb{E}\left[\left(\left(\begin{array}{c}
h_{l}\\
1
\end{array}\right)\otimes\mathbf{I}\right)\left(1\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right)\left(\left(\begin{array}{cc}
h_{l}^{T} &amp; 1\end{array}\right)\otimes\mathbf{I}\right)\right]\label{eq:befsim}\\
 &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{c}
h_{l}\\
1
\end{array}\right)\left(\begin{array}{cc}
h_{l}^{T} &amp; 1\end{array}\right)\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right]\label{eq:aftsim}\\
 &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc}
h_{l}h_{l}^{T} &amp; h_{l}\\
h_{l}^{T} &amp; 1
\end{array}\right)\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right]\nonumber 
\end{eqnarray}\]

<p>In eq <a class="Reference" href="#eq:befsim">\ref{eq:befsim}</a> we added \(1\otimes\) as it does not change anything. Between eq <a class="Reference" href="#eq:befsim">\ref{eq:befsim}</a> and <a class="Reference" href="#eq:aftsim">\ref{eq:aftsim}</a> we used the fact that \(\left(A\otimes B\right)\left(C\otimes D\right)=AC\otimes BD\) when \(A,B,C,D\) have corresponding sizes (i.e. the products \(AC\) and \(BD\) make sense).</p>

<h3 id="discussion">Discussion</h3>

<p>We obtained an <i>exact</i> expression for the block corresponding to layer \(l\):</p>

\[\begin{eqnarray}
G_{l} &amp; = &amp; \mathbb{E}\left[\underbrace{\left(\begin{array}{cc}
h_{l}h_{l}^{T} &amp; h_{l}\\
h_{l}^{T} &amp; 1
\end{array}\right)}_{\left(1\right)}\otimes\underbrace{J_{a_{l}}^{T}DJ_{a_{l}}}_{\left(2\right)}\right]\label{eq:exact}\\
 &amp; = &amp; \left(\begin{array}{cc}
\mathbb{E}\left[h_{l}h_{l}^{T}\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right] &amp; \mathbb{E}\left[h_{l}\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right]\\
\mathbb{E}\left[h_{l}^{T}\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right] &amp; \mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]
\end{array}\right)
\end{eqnarray}\]

<p>It is an expectation of Kronecker products. Note that we can not swap the expectation and the Kronecker products, and thus while the expression in eq <a class="Reference" href="#eq:exact">\ref{eq:exact}</a> is exact, the one used in KFAC <span class="bibcites">[<a class="bibliocite" name="cite-2" href="#biblio-2"><span class="bib-index">2</span></a>]</span> is an approximation.</p>

<p>In eq <a class="Reference" href="#eq:exact">\ref{eq:exact}</a> we denoted by \(\left(2\right)\) the contribution that is backpropagated, and by \(\left(1\right)\) a contribution that is local to the parameters of the layer.</p>

<h2 id="inverting-the-matrix">Inverting the matrix</h2>

<h3 id="kfac-drill-down">KFAC drill-down</h3>

<p>Exactly inverting this matrix can still be untractable for typical neural networks. An approximation that is easier to manipulate is proposed in KFAC <span class="bibcites">[<a class="bibliocite" name="cite-2" href="#biblio-2"><span class="bib-index">2</span></a>]</span>. The key property that we are after here is that for 2 invertible matrices \(A\) and \(B\) we have that \(\left(A\otimes B\right)^{-1}=A^{-1}\otimes B^{-1}\). It becomes:</p>

\[\begin{eqnarray*}
G_{l} &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc}
h_{l}h_{l}^{T} &amp; h_{l}\\
h_{l}^{T} &amp; 1
\end{array}\right)\otimes J_{a_{l}}^{T}DJ_{a_{l}}\right]\\
 &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc}
h_{l}h_{l}^{T} &amp; h_{l}\\
h_{l}^{T} &amp; 1
\end{array}\right)\right]\otimes\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]+R\\
 &amp; \approx &amp; \mathbb{E}\left[\left(\begin{array}{cc}
h_{l}h_{l}^{T} &amp; h_{l}\\
h_{l}^{T} &amp; 1
\end{array}\right)\right]\otimes\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]
\end{eqnarray*}\]

<p>The residual \(R\) resembles a covariance between both terms:</p>

\[\begin{eqnarray*}
R &amp; = &amp; \mathbb{E}\left[\left(\left(\begin{array}{cc}
h_{l}h_{l}^{T} &amp; h_{l}\\
h_{l}^{T} &amp; 1
\end{array}\right)-\mathbb{E}\left[\left(\begin{array}{cc}
h_{l}h_{l}^{T} &amp; h_{l}\\
h_{l}^{T} &amp; 1
\end{array}\right)\right]\right)\otimes\left(J_{a_{l}}^{T}DJ_{a_{l}}-\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]\right)\right]\\
 &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc}
h_{l}h_{l}^{T}-\mathbb{E}\left[h_{l}h_{l}^{T}\right] &amp; h_{l}-\mathbb{E}\left[h_{l}\right]\\
h_{l}^{T}-\mathbb{E}\left[h_{l}\right] &amp; 0
\end{array}\right)\otimes\left(J_{a_{l}}^{T}DJ_{a_{l}}-\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]\right)\right]
\end{eqnarray*}\]

<p>The conditions under which it is negligible have not been extensively studied, or at least published to the best of our knowledge. We can however remark that if one part is close to \(0\) then the expected product will be small. This is achieved for instance if \(\left(J_{a_{l}}^{T}DJ_{a_{l}}-\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]\right)\) is small for all \(x\sim p\left(x\right)\) the data generating distribution (recall that \(D\) and \(J_{a_{l}}\) depend on \(x\)). To put it into words if the value of \(J_{a_{l}}^{T}DJ_{a_{l}}\) does not vary much for all training examples. By symmetry we can make a similar argument for \(\left(h_{l}h_{l}^{T}-\mathbb{E}\left[h_{l}h_{l}^{T}\right]\right)\).</p>

<h3 id="kfac-inversion-">KFAC inversion<a class="Label" name="subsec:KFAC-inversion"> </a></h3>

<p>We now have a factorized approximate expression for \(G_{l}\):</p>

\[\begin{eqnarray*}
G_{l}^{\text{approx}} &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{cc}
h_{l}h_{l}^{T} &amp; h_{l}\\
h_{l}^{T} &amp; 1
\end{array}\right)\right]\otimes\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]\\
 &amp; = &amp; \left(\begin{array}{cc}
\mathbb{E}\left[h_{l}h_{l}^{T}\right] &amp; \mathbb{E}\left[h_{l}\right]\\
\mathbb{E}\left[h_{l}^{T}\right] &amp; 1
\end{array}\right)\otimes\mathbb{E}\left[J_{a_{l}}^{T}DJ_{a_{l}}\right]\\
 &amp; = &amp; A\otimes B
\end{eqnarray*}\]

<p>Note that while the derivations proposed in KFAC use a single vector \(\theta\) for all parameters of the layer \(l\), we explicitely separated the weight matrix \(W\) and the bias \(b\) in section <a class="Reference" href="#subsec:Expressions-for-the">2.2↑</a> which gives a slightly different expression. Thus the matrix \(A\) is separated into 2 blocks: 2 blocks on the diagonal that correspond to the weight matrix (block \(1,1\)) and the bias (block \(2,2\)), and 2 cross-terms that explicit their interactions.</p>

<p>We will see that separating the bias gives a nicer interpretation with a covariance matrix (as opposed to non-centered statistics).</p>

<p>We can now use the property \(\left(G_{l}^{\text{approx}}\right)^{-1}=A^{-1}\otimes B^{-1}\). \(B^{-1}\) can not be be further simplified, so the next part is to obtain an expression for \(A^{-1}\):</p>

\[\begin{eqnarray*}
A^{-1} &amp; = &amp; \left(\begin{array}{cc}
\mathbb{E}\left[h_{l}h_{l}^{T}\right] &amp; \mathbb{E}\left[h_{l}\right]\\
\mathbb{E}\left[h_{l}^{T}\right] &amp; 1
\end{array}\right)^{-1}
\end{eqnarray*}\]

<p>We can use the formula for inverting a block matrix (see <a class="URL" href="https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_inversion">Wikipedia:Block Matrix</a>). We denote by \(C=\mathbb{E}\left[h_{l}h_{l}^{T}\right]-\mathbb{E}\left[h_{l}\right]\mathbb{E}\left[h_{l}^{T}\right]\) and we get:</p>

\[\begin{eqnarray*}
A^{-1} &amp; = &amp; \left(\begin{array}{cc}
C^{-1} &amp; -C^{-1}\mathbb{E}\left[h_{l}\right]\\
-\mathbb{E}\left[h_{l}^{T}\right]C^{-1} &amp; 1+\mathbb{E}\left[h_{l}^{T}\right]C^{-1}\mathbb{E}\left[h_{l}\right]
\end{array}\right)
\end{eqnarray*}\]

<p>Note that \(C\) is the covariance of \(h_{l}\): \(C=\mathbb{E}\left[\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)^{T}\right]=cov\left(h_{l}\right)\). It is centered (we substract \(\mathbb{E}\left[h_{l}\right]\)) which follows from the block matrix inversion formula, which in turns follows from the fact that we separated the bias. This motivates the use of centered statistics in second order inspired algorithms.</p>

<h2 id="writing-the-update">Writing the update</h2>

<h3 id="derivation">Derivation</h3>

<p>Now that we have an expression for \(G_{l}^{-1}\) we can write the product \(G_{l}^{-1}\nabla_{\theta_{l}}\mathcal{L}\) required to make an update \(\theta_{l}^{t+1}=\theta_{l}^{t}-\lambda G_{l}^{-1}\nabla_{\theta_{l}}\mathcal{L}\). In section <a class="Reference" href="#subsec:Expressions-for-the">2.2↑</a> we wrote an expression for the jacobians \(J_{l}=\frac{\partial f\left(x;\theta\right)}{\partial\theta_{l}}\). By a similar analysis we can write the gradients \(\nabla_{\theta_{l}}\mathcal{L}\):</p>

\[\begin{eqnarray*}
\nabla_{\theta_{l}}\mathcal{L} &amp; = &amp; \mathbb{E}\left[\nabla_{\theta_{l}}\ell\left(x\right)\right]\\
 &amp; = &amp; \mathbb{E}\left[\left(\frac{\partial\ell\left(x\right)}{\partial\theta_{l}}\right)^{T}\right]\\
 &amp; = &amp; \mathbb{E}\left[\left(\frac{\partial\ell\left(x\right)}{\partial a_{l}}\frac{\partial a_{l}}{\partial\theta_{l}}\right)^{T}\right]\\
 &amp; = &amp; \mathbb{E}\left[\left(\frac{\partial a_{l}}{\partial\theta_{l}}\right)^{T}\left(\frac{\partial\ell\left(x\right)}{\partial a_{l}}\right)^{T}\right]\\
 &amp; = &amp; \mathbb{E}\left[\left(\frac{\partial a_{l}}{\partial\theta_{l}}\right)^{T}\nabla_{a_{l}}\ell\left(x\right)\right]
\end{eqnarray*}\]

<p>Using the same expressions as in <a class="Reference" href="#subsec:Expressions-for-the">2.2↑</a> we can simplify \(\frac{\partial a_{l}}{\partial\theta_{l}}=\left(\left(\begin{array}{cc}
h_{l}^{T} &amp; 1\end{array}\right)\otimes\mathbf{I}\right)\):</p>

\[\begin{eqnarray*}
\nabla_{\theta_{l}}\mathcal{L} &amp; = &amp; \mathbb{E}\left[\left(\left(\begin{array}{c}
h_{l}\\
1
\end{array}\right)\otimes\mathbf{I}\right)\nabla_{a_{l}}\ell\left(x\right)\right]\\
 &amp; = &amp; \mathbb{E}\left[\left(\begin{array}{c}
h_{l}\\
1
\end{array}\right)\otimes\nabla_{a_{l}}\ell\left(x\right)\right]
\end{eqnarray*}\]

<p>Multiplying together with \(\left(G_{l}^{\text{approx}}\right)^{-1}\) we get the product \(\Delta_{\theta_{l}}=\left(G_{l}^{\text{approx}}\right)^{-1}\nabla_{\theta_{l}}\mathcal{L}\):</p>

\[\begin{eqnarray*}
\Delta_{\theta_{l}} &amp; = &amp; \left(A^{-1}\otimes B^{-1}\right)\nabla_{\theta_{l}}\mathcal{L}\\
 &amp; = &amp; \left(\left(\begin{array}{cc}
C^{-1} &amp; -C^{-1}\mathbb{E}\left[h_{l}\right]\\
-\mathbb{E}\left[h_{l}^{T}\right]C^{-1} &amp; 1+\mathbb{E}\left[h_{l}^{T}\right]C^{-1}\mathbb{E}\left[h_{l}\right]
\end{array}\right)\otimes B^{-1}\right)\mathbb{E}\left[\left(\begin{array}{c}
h_{l}\\
1
\end{array}\right)\otimes\nabla_{a_{l}}\ell\left(x\right)\right]\\
 &amp; = &amp; \left(\begin{array}{c}
C^{-1}\mathbb{E}\left[\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\otimes B^{-1}\nabla_{a_{l}}\ell\left(x\right)\right]\\
\mathbb{E}\left[\left(1-\mathbb{E}\left[h_{l}^{T}\right]C^{-1}\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\right)B^{-1}\nabla_{a_{l}}\ell\left(x\right)\right]
\end{array}\right)
\end{eqnarray*}\]

<p>In the first line we can read the update for \(W\) (in fact its vectorized version \(vec\left(W\right)\)), and the second line is the update for \(b\).</p>

<h3 id="updating-the-weight-matrix-w-">Updating the weight matrix \(W\)<a class="Label" name="subsec:Updating-the-weight"> </a></h3>

<p>The new update for \(W\) is given by:</p>

\[\begin{eqnarray*}
\Delta_{\text{vec}\left(W_{l}\right)} &amp; = &amp; C^{-1}\mathbb{E}\left[\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\otimes B^{-1}\nabla_{a_{l}}\ell\left(x\right)\right]
\end{eqnarray*}\]

<p>We some algebraic manipulations we get back to the expression for the unflattened matrix:\(\begin{eqnarray*}
\Delta_{\text{vec}\left(W_{l}\right)} &amp; = &amp; \left(C^{-1}\otimes B^{-1}\right)\mathbb{E}\left[\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\otimes\nabla_{a_{l}}\ell\left(x\right)\right]\\
 &amp; = &amp; \left(C^{-1}\otimes B^{-1}\right)\mathbb{E}\left[\text{vec}\left(\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\otimes\nabla_{a_{l}}\ell\left(x\right)\right)\right]\\
\Delta_{W_{l}} &amp; = &amp; B^{-1}\mathbb{E}\left[\nabla_{a_{l}}\ell\left(x\right)\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)^{T}\right]C^{-1}
\end{eqnarray*}\)</p>

<p>This is to compare with the usual gradient descent update given by:</p>

\[\begin{eqnarray*}
\nabla_{W_{l}}\mathcal{L} &amp; = &amp; \mathbb{E}\left[\nabla_{a_{l}}\ell\left(x\right)h_{l}^{T}\right]
\end{eqnarray*}\]

<p>We can notice 2 additions:</p>

<ul>
<li>
the update is rescaled and rotated using the 2 matrices \(B^{-1}\) and \(C^{-1}\)
</li>
<li>
the expectation is centered by substracting \(\mathbb{E}\left[h_{l}\right]\)
</li>

</ul>

<p>In addition to the derivation proposed in KFAC, by expliciting the bias we obtained 2 different centerings:</p>

<ul>
<li>
the covariance matrix \(C\)
</li>
<li>
the expectation is centered by substracting \(\mathbb{E}\left[h_{l}\right]\)
</li>

</ul>

<p>### Updating the bias vector \(b\)</p>

<p>The new update for \(b\) is given by:</p>

\[\begin{eqnarray*}
\Delta_{b_{l}} &amp; = &amp; \mathbb{E}\left[\left(1-\mathbb{E}\left[h_{l}^{T}\right]C^{-1}\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\right)B^{-1}\nabla_{a_{l}}\ell\left(x\right)\right]
\end{eqnarray*}\]

<p>This is to compare with the usual gradient descent update given by:</p>

\[\begin{eqnarray*}
\nabla_{b_{l}}\mathcal{L} &amp; = &amp; \mathbb{E}\left[\nabla_{a_{l}}\ell\left(x\right)\right]
\end{eqnarray*}\]

<p>Once again we notice that the update is scaled and rotated using \(B^{-1}\) but there is also this strange scalar scaling \(\left(1-\mathbb{E}\left[h_{l}^{T}\right]C^{-1}\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\right)\) for which we are not able to give an interpretation. However in practice we noted that we did not have any performance gain compared to using only \(1\).</p>

<h2 id="conclusion">Conclusion</h2>

<p>We gave explicit derivation of the second order updates used in Gauss-Newton and in Natural gradient. By explicitly separating the weight matrices and the bias we obtained a nice centering term in both the covariance matrix used to rotate the update \(C=\mathbb{E}\left[\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)^{T}\right]\), and in the expectation used to compute the gradient \(\mathbb{E}\left[\nabla_{a_{l}}\ell\left(x\right)\left(h_{l}-\mathbb{E}\left[h_{l}\right]\right)^{T}\right]\).</p>

<p>It is well-known that centering things is often useful. It is sometimes referred as the centering trick, or mean only batch norm. Another efficient technique called natural neural networks <span class="bibcites">[<a class="bibliocite" name="cite-1" href="#biblio-1"><span class="bib-index">1</span></a>]</span> building on the structure of the FIM mentions the trick without giving it much justification. To the best that we know this justification based on the structure of the second order matrices has not yet been contributed. We hope that this blog note can enlighten deep learning practitioners who are not very familliar with second order methods, in order to invent new approximate algorithms with more efficient updates.</p>

<h2 id="references">References</h2>

<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-1"><span class="bib-index">1</span></a>] </span> <span class="bib-authors">Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, others</span>. <span class="bib-title">Natural neural networks</span>.  <i><span class="bib-booktitle">Advances in Neural Information Processing Systems</span></i>:<span class="bib-pages">2071—2079</span>, <span class="bib-year">2015</span>.

<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-2"><span class="bib-index">2</span></a>] </span> <span class="bib-authors">James Martens, Roger Grosse</span>. <span class="bib-title">Optimizing neural networks with kronecker-factored approximate curvature</span>.  <i><span class="bib-booktitle">International Conference on Machine Learning</span></i>:<span class="bib-pages">2408—2417</span>, <span class="bib-year">2015</span>.

<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-3"><span class="bib-index">3</span></a>] </span> <span class="bib-authors">Razvan Pascanu, Yoshua Bengio</span>. <span class="bib-title">Revisiting natural gradient for deep networks</span>. <i><span class="bib-journal">arXiv preprint arXiv:1301.3584</span></i>, <span class="bib-year">2013</span>.




</p></p></p>]]></content><author><name></name></author><category term="note" /><summary type="html"><![CDATA[This note gives the derivations for the inverse of 2 different but related 2nd order matrices: the Fisher Information Matrix, and the Gauss-Newton approximation of the Hessian. In particular we highlight 2 centering properties that follow from the local structure of those matrices:]]></summary></entry></feed>